# Logistic Regression

## å›é¡¾ï¼šè´å¶æ–¯è§†è§’ä¸‹çš„å›å½’æ¨¡å‹

åœ¨è´å¶æ–¯ç»Ÿè®¡æ¡†æ¶ä¸‹ï¼Œå›å½’æ¨¡å‹çš„æ„å»ºä¸æ£€éªŒæ–¹æ³•ä¸ä¼ ç»Ÿé¢‘ç‡å­¦æ´¾æœ‰æ‰€ä¸åŒã€‚

- åœ¨è´å¶æ–¯å›å½’ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°è¢«è§†ä¸ºéšæœºå˜é‡ï¼Œé€šè¿‡æ•°æ®æ¥æ›´æ–°å…¶æ¦‚ç‡åˆ†å¸ƒã€‚

- è´å¶æ–¯æ–¹æ³•é€šè¿‡**å¯¹å‚æ•°çš„åéªŒåˆ†å¸ƒè¿›è¡Œæ¨æ–­**ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹çš„é€‚åº”æ€§ä¸æ˜¾è‘—æ€§ã€‚

- è¿™ç§æ–¹æ³•ä½¿å¾—æˆ‘ä»¬ä¸ä»…èƒ½å¾—åˆ°å‚æ•°çš„ç‚¹ä¼°è®¡ï¼Œè¿˜èƒ½è·å¾—å…³äºè¿™äº›å‚æ•°çš„ä¸ç¡®å®šæ€§çš„ä¿¡æ¯ã€‚


åœ¨ä¹‹å‰çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»¥è‡ªæˆ‘ä¼˜åŠ¿åŒ¹é…èŒƒå¼ä¸ºä¾‹ï¼Œå»ºç«‹äº†ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼š

$$
RT_{sec} \sim \mathcal{N}(\beta_0+\beta_1Â·Label, \sigma^2)
$$

- åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œååº”æ—¶ï¼ˆRTï¼‰æ˜¯ä¸€ä¸ªè¿ç»­çš„å› å˜é‡ã€‚

ç„¶è€Œï¼Œåœ¨è®¸å¤šå¿ƒç†å­¦ç ”ç©¶ä¸­ï¼Œå¦ä¸€ä¸ªæœ€å¸¸è§çš„å› å˜é‡å°±æ˜¯**ååº”æ˜¯å¦æ­£ç¡®**ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ª**äºŒåˆ†å˜é‡ï¼ˆæ­£ç¡® / é”™è¯¯ï¼‰**ã€‚

ğŸ¤”**æ€è€ƒï¼šå½“å› å˜é‡æ˜¯äºŒåˆ†å˜é‡æ—¶ï¼Œä¼ ç»Ÿçš„çº¿æ€§å›å½’æ¨¡å‹æ˜¯å¦ä»ç„¶é€‚ç”¨å‘¢ï¼Ÿ**

## ä»¥éšæœºç‚¹è¿åŠ¨ä»»åŠ¡ä¸ºä¾‹ï¼šè´å¶æ–¯é€»è¾‘å›å½’

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»¥ä¹‹å‰ä»‹ç»è¿‡çš„éšæœºç‚¹è¿åŠ¨ä»»åŠ¡ï¼ˆRandom Motion Dot Taskï¼‰ä¸ºä¾‹ï¼Œæ¥ç†è§£è´å¶æ–¯é€»è¾‘å›å½’æ¨¡å‹çš„åº”ç”¨ã€‚

- åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œå‚ä¸è€…è§‚å¯Ÿå±å¹•ä¸Šéšæœºè¿åŠ¨çš„ç‚¹ï¼Œ**è¿™äº›ç‚¹çš„è¿åŠ¨æ–¹å‘å…·æœ‰ä¸€å®šçš„ä¸€è‡´æ€§ï¼ˆå³å¤§éƒ¨åˆ†ç‚¹æœæŸä¸€æ–¹å‘ç§»åŠ¨ï¼‰**ã€‚

- å‚ä¸è€…çš„ä»»åŠ¡æ˜¯**åˆ¤æ–­è¿™äº›ç‚¹çš„ä¸»è¦ç§»åŠ¨æ–¹å‘ï¼ˆä¾‹å¦‚ï¼Œå‘å·¦è¿˜æ˜¯å‘å³ï¼‰**ã€‚

- å®éªŒè®¾è®¡å¯ä»¥æ§åˆ¶ç‚¹çš„**è¿åŠ¨ä¸€è‡´æ€§ï¼ˆå¦‚10%æˆ–40%ï¼‰**ï¼Œä»è€Œå½±å“å‚ä¸è€…ä½œå‡ºæ­£ç¡®å†³ç­–çš„éš¾åº¦ã€‚

***å’Œä¹‹å‰çš„å†…å®¹ä¸åŒï¼Œæœ¬æ¬¡è¯¾æˆ‘ä»¬å°†ç ”ç©¶ç‚¹çš„è¿åŠ¨ä¸€è‡´æ€§ä¸åˆ¤æ–­æ˜¯å¦æ­£ç¡®ä¹‹é—´çš„å…³ç³»ï¼š***

1.è‡ªå˜é‡ï¼šç‚¹çš„è¿åŠ¨ä¸€è‡´æ€§ï¼ˆ10%ä¸€è‡´æ€§ï¼Œ40%ä¸€è‡´æ€§ï¼‰

2.å› å˜é‡ï¼šåˆ¤æ–­æ˜¯å¦æ­£ç¡®ï¼ˆå³è¢«è¯•æ˜¯å¦å‡†ç¡®åˆ¤æ–­äº†ç‚¹çš„ä¸»è¦è¿åŠ¨æ–¹å‘ï¼Œ1ä»£è¡¨ååº”æ­£ç¡®ï¼Œ0ä»£è¡¨ååº”é”™è¯¯ï¼‰

**æˆ‘ä»¬æœ¬æ¬¡å°†ç ”ç©¶ç‚¹çš„è¿åŠ¨ä¸€è‡´æ€§ä¸åˆ¤æ–­æ˜¯å¦æ­£ç¡®ä¹‹é—´çš„å…³ç³»ï¼š**

1.è‡ªå˜é‡ï¼šç‚¹çš„è¿åŠ¨ä¸€è‡´æ€§ï¼ˆ10%ä¸€è‡´æ€§å’Œ40%ä¸€è‡´æ€§ï¼‰

2.å› å˜é‡ï¼šåˆ¤æ–­æ˜¯å¦æ­£ç¡®å³è¢«è¯•æ˜¯å¦å‡†ç¡®åˆ¤æ–­äº†ç‚¹çš„ä¸»è¦è¿åŠ¨æ–¹å‘ï¼Œ1ä»£è¡¨ååº”æ­£ç¡®ï¼Œ0ä»£è¡¨ååº”é”™è¯¯ï¼‰

ä»¥Evans et al.ï¼ˆ2020, Exp. 1ï¼‰ çš„æ•°æ®ä¸ºä¾‹è¿›è¡Œæ¢ç´¢ã€‚

> Evans, N. J., Hawkins, G. E., & Brown, S. D. (2020). The role of passing time in decision-making. Journal of Experimental Psychology: Learning, Memory, and Cognition, 46(2), 316â€“326. https://doi.org/10.1037/xlm0000725

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ•°æ®å¹¶æŸ¥çœ‹å…¶åˆ†å¸ƒæƒ…å†µï¼š

```python
# å¯¼å…¥ pymc æ¨¡å‹åŒ…ï¼Œå’Œ arviz ç­‰åˆ†æå·¥å…· 
import pymc as pm
import arviz as az
import seaborn as sns
import scipy.stats as st
import numpy as np
import matplotlib.pyplot as plt
import xarray as xr
import pandas as pd
import ipywidgets

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")
```

```python
# ä½¿ç”¨ pandas å¯¼å…¥ç¤ºä¾‹æ•°æ®
try:
  df = pd.read_csv("/home/mw/input/bayes3797/evans2020JExpPsycholLearn_exp1_full_data.csv") 
except:
  df = pd.read_csv('data/evans2020JExpPsycholLearn_exp1_full_data.csv')

# ç­›é€‰ç¼–å·ä¸º 31727 çš„æ•°æ®ï¼Œå¹¶ä¸”ç­›é€‰å‡ºä¸¤ä¸ªä¸åŒçš„ percentCoherence
df_clean = df[(df['subject'] == 31727) & (df['percentCoherence'].isin([10, 40]))]
df_clean  = df_clean[["subject", "percentCoherence", "correct"]]

df_clean
```

```python
df_clean.groupby("percentCoherence").correct.mean()
```
output:
percentCoherence
10    0.686047
40    0.926316
Name: correct, dtype: float64

```python
# å› å˜é‡åˆ†å¸ƒ
ax = df_clean.groupby("percentCoherence").correct.mean().plot.bar()

ax.set_ylabel("accuracy")
sns.despine()
plt.show()
```

![alt text](image-8.png)

é€šè¿‡æ•£ç‚¹å›¾ï¼Œæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°è§‚å¯Ÿåˆ°æ•°æ®ä¸­ä¸åŒå˜é‡çš„åˆ†å¸ƒæƒ…å†µï¼š

![alt text](image-9.png)

åœ¨ç¤ºä¾‹æ•°æ®ä¸­ä¸­ï¼Œ

- **å› å˜é‡â€œcorrectâ€æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»å˜é‡**ï¼Œè¡¨ç¤ºè¢«è¯•æ˜¯å¦è·å¾—æ­£ç¡®ååº”ã€‚

- æˆ‘ä»¬è€ƒè™‘çš„**è‡ªå˜é‡â€œpercentCoherenceâ€å¯ä»¥æ˜¯è¿ç»­å˜é‡**ï¼Œè¡¨ç¤ºè¢«è¯•çš„åˆºæ¿€å¼ºåº¦ (percentCoherence æˆ– motion strength)ã€‚

- æˆ‘ä»¬æ„Ÿè§‰å…´è¶£çš„æ˜¯â€œcorrectâ€ä¸â€œpercentCoherenceâ€ä¹‹é—´çš„å…³ç³»ã€‚

## é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦ä½¿ç”¨çº¿æ€§å›å½’åˆ†ææ­£ç¡®ç‡æ•°æ®ï¼Ÿ

- åœ¨æˆ‘ä»¬ä¼ ç»Ÿçš„è®¤çŸ¥å®éªŒç ”ç©¶çš„æ•°æ®åˆ†æä¸­ï¼Œä¸€èˆ¬æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨**tæ£€éªŒæˆ–è€…æ–¹å·®åˆ†æ**ï¼ˆçº¿æ€§å›å½’æ¨¡å‹çš„ç‰¹ä¾‹ï¼‰å¯¹æ­£ç¡®ç‡è¿›è¡Œåˆ†æï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„â€œ**éµå¾ªé¢†åŸŸå†…çš„ä¼ ç»Ÿ**â€ã€‚

  - ç„¶è€Œï¼Œè¿™æ ·å¤„ç†çš„å‰ææ˜¯**å°†æ­£ç¡®ç‡ä½œä¸ºä¸€ä¸ªè¿ç»­æ•°æ®çœ‹å¾…**ï¼Œå¹¶é‡‡ç”¨çº¿æ€§å›å½’æ¨¡å‹å¯¹å®ƒè¿›è¡Œåˆ†æï¼Œè¿™å®é™…ä¸Šä¸¥æ ¼æ¥è¯´æ˜¯å­˜åœ¨é—®é¢˜çš„ã€‚

***1ã€æˆ‘ä»¬èƒ½å¦å¯¹æ¯ä¸€ä¸ªè¯•æ¬¡çš„æ•°æ®è¿›è¡Œåˆ†æï¼Ÿ***

å¦‚æœåªæ˜¯å°†æ‰€æœ‰è¯•æ¬¡çš„æ•°æ®æ±‚ä¸€ä¸ªå¹³å‡çš„è¯ï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰ä½“ç°å‡ºæ•°æ®é‡çš„å¤§å°æ‰€å¸¦æ¥çš„å½±å“ï¼ˆä¸¢å¤±äº†è¿™ä¸€éƒ¨åˆ†çš„ä¿¡æ¯ï¼‰ï¼Œä¾‹å¦‚åœ¨ä¹‹å‰è´å¶æ–¯åŸºæœ¬åŸç†çš„è¯¾ç¨‹ä¸­æ‰€ä»‹ç»çš„â€œæ•°æ®ä¸ä¸€æ ·ï¼ŒåéªŒæ›´æ–°ä¹Ÿä¼šä¸åŒâ€ã€‚

***2ã€å¦‚æœæˆ‘ä»¬ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹å¯¹æ­£ç¡®ç‡è¿›è¡Œåˆ†æï¼Œä¼šå­˜åœ¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ***

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹èµ·å› å˜é‡y(å–å€¼ä¸º0æˆ–1)å’Œè‡ªå˜é‡xçš„çº¿æ€§å…³ç³»ï¼š

$$
y_i = \beta_0+\beta_1*x_i \\
or\\
Y_i \sim N(\mu,\sigma),\mu=\beta_0+\beta_1*x_i
$$

- å­˜åœ¨çš„é—®é¢˜ï¼š

  - çº¿æ€§å›å½’æ¨¡å‹ä¸­çš„å› å˜é‡yæœä»ä¸€ä¸ªæ­£æ€åˆ†å¸ƒï¼Œå®ƒçš„ç†è®ºå–å€¼åº”å½“æ˜¯è´Ÿæ— ç©·åˆ°æ­£æ— ç©·ï¼Œä½†æ˜¯æˆ‘ä»¬æ¯ä¸ªè¯•æ¬¡çš„å–å€¼åªèƒ½ä¸º0æˆ–1ï¼Œå¹¶ä¸æ»¡è¶³yçš„ç†è®ºå–å€¼ã€‚

***3ã€å¦‚æœå¯¹äºŒåˆ†å˜é‡çš„æ¨¡å‹å‚æ•°è¿›è¡Œå›å½’åˆ†æï¼Œä¼šå­˜åœ¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ***

- æ­£ç¡®ååº”çš„æ¦‚ç‡å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå³$Y_i|\pi_i \sim Bern(\pi_i)$ï¼Œå…¶ä¸­$\pi_i$æ˜¯åœ¨åˆºæ¿€å¼ºåº¦ï¼ˆè¿åŠ¨ä¸€è‡´æ€§ï¼‰$x_i$ä¸‹è¢«è¯•è·å¾—æ­£ç¡®ååº”çš„æ¦‚ç‡ã€‚

- æ›²çº¿ä¸Šçš„æ¯ä¸€ä¸ªç‚¹éƒ½æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒ$Y_i|\pi_i \sim Bern(\pi_i)$ã€‚

![alt text](image.png)

- å­˜åœ¨çš„é—®é¢˜ï¼š

æˆ‘ä»¬åœ¨åšåéªŒé¢„æµ‹æ—¶ä»ç„¶å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºä»ç„¶æ˜¯æ ¹æ®æ­£æ€åˆ†å¸ƒå¯¹$\pi$è¿›è¡ŒåéªŒé¢„æµ‹ï¼Œæ­¤æ—¶è¿˜æ˜¯ä¼šå¾ˆå®¹æ˜“å‡ºç°0-1åŒºé—´ä»¥å¤–çš„å–å€¼ã€‚

- è‡³æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ–°æ¦‚å¿µâ€”â€”â€”å‘ç”Ÿæ¯”ï¼ˆoddsï¼‰

## Probability & odds

ä¸æ¦‚ç‡ä¸åŒï¼Œå‘ç”Ÿæ¯”ï¼ˆoddsï¼‰æè¿°çš„æ˜¯**äº‹ä»¶å‘ç”Ÿæ¦‚ç‡**ä¸**äº‹ä»¶ä¸å‘ç”Ÿæ¦‚ç‡**ä¹‹æ¯”ï¼Œè€Œæ¦‚ç‡æè¿°çš„æ˜¯äº‹ä»¶å‘ç”Ÿçš„ç»å¯¹å¯èƒ½æ€§ã€‚

$$
odds=\frac{\pi}{1-\pi},\pi=\frac{odds}{1+odds}
$$

- $\pi$ä¸ºå› å˜é‡Yå‘ç”Ÿçš„æ¦‚ç‡

ä¸¾ä¾‹ï¼šæ˜å¤©æ˜¯å¦ä¼šä¸‹é›¨ï¼Ÿ

- æˆ‘ä»¬å‡è®¾æ˜å¤©ä¸‹é›¨å‘ç”Ÿçš„æ¦‚ç‡æ˜¯$\pi=2/3$ï¼Œé‚£ä¹ˆæ˜å¤©ä¸ä¸‹é›¨çš„æ¦‚ç‡ä¸º$1-\pi=1/3$

$$
odds~of~rain=\frac{2/3}{1-2/3}=2
$$

- $\pi$çš„å–å€¼ä¸ºï¼ˆ0ï¼Œ1ï¼‰ï¼Œoddsçš„å–å€¼ä¸º[0ï¼Œ+âˆï¼‰

å°†å‘ç”Ÿæ¯”ä¸1è¿›è¡Œæ¯”è¾ƒæ¥è¡¡é‡äº‹ä»¶å‘ç”Ÿçš„ä¸ç¡®å®šæ€§ï¼š

1.å½“äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡$\pi<0.5$æ—¶ï¼Œäº‹ä»¶çš„å‘ç”Ÿæ¯”å°äº1

2.å½“äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡$\pi=0.5$æ—¶ï¼Œäº‹ä»¶çš„å‘ç”Ÿæ¯”ç­‰äº1

3.å½“äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡$\pi>0.5$æ—¶ï¼Œäº‹ä»¶çš„å‘ç”Ÿæ¯”å¤§äº1

- è™½ç„¶æˆ‘ä»¬å°†$\pi$çš„å–å€¼èŒƒå›´æ‰©å¤§åˆ°äº†[0, +âˆï¼‰ï¼Œä½†ä»ç„¶æ— æ³•æ»¡è¶³ï¼ˆ-âˆï¼Œ+âˆï¼‰çš„å–å€¼èŒƒå›´ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ç»§ç»­è¿›è¡Œè½¬æ¢ã€‚

***è¿›ä¸€æ­¥å¯¹oddsè¿›è¡Œè½¬æ¢ï¼Œè®©å…¶åœ¨æ­£è´Ÿæ— ç©·ä¸Šå‡æœ‰å–å€¼â€”â€”logï¼ˆoddsï¼‰***

$$
log(odds_i)=\beta_0+\beta_1X_{i1}
$$

- logåï¼Œå–å€¼èŒƒå›´ç¬¦åˆï¼ˆ-âˆï¼Œ+âˆï¼‰ã€‚

æ€»ç»“èµ·æ¥ï¼š

$$
y_i\sim Bern(\pi)\\
odds=\frac{\pi}{1-\pi}\\
log(odds)=\beta_0+\beta_1X
$$

- åœ¨å¹¿ä¹‰çº¿æ€§æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¿æ¥å‡½æ•°(link function)g(â‹…)ï¼Œä½¿å¾—å‚æ•°$g(\pi_i)$å¯ä»¥è¢«è¡¨ç¤ºä¸ºè‡ªå˜é‡$X_{i1}$çš„çº¿æ€§ç»„åˆã€‚

![alt text](image-1.png)

- æ€»çš„æ¥è¯´ï¼Œå¯¹äºå…¶ä»–çš„æ•°æ®ç±»å‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å…ˆæ‰¾åˆ°ç¬¦åˆå…¶ç‰¹ç‚¹çš„ç»Ÿè®¡åˆ†å¸ƒä»¥åŠè¿™ç§åˆ†å¸ƒçš„å‚æ•°ï¼Œç„¶åå¯¹å®ƒè¿›è¡Œç›¸åº”çš„è½¬æ¢å¹¶ç¬¦åˆæˆ‘ä»¬å›å½’æ¨¡å‹çš„ç‰¹å¾ï¼Œæœ€ååŸºäºè¿æ¥å‡½æ•°å»ºç«‹èµ·è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚

### å¹¿ä¹‰çº¿æ€§æ¨¡å‹(Generalized Linear Model, GLM)

- å¯¹çº¿æ€§å›å½’æ¨¡å‹çš„æ¨å¹¿

- åœ¨å› å˜é‡ä¸æ»¡è¶³çº¿æ€§æ¨¡å‹çš„é¢„è®¾æ¡ä»¶æ—¶ï¼Œä»ç„¶ä½¿ç”¨çº¿æ€§æ¨¡å‹çš„æ€è·¯ã€‚

- æ ¸å¿ƒåœ¨äºé€šè¿‡è¿æ¥å‡½æ•°å¯¹å› å˜é‡è¿›è¡Œå˜æ¢ï¼Œä½¿å…¶æ»¡è¶³çº¿æ€§æ¨¡å‹çš„æ¡ä»¶ã€‚

- **å¯¹äºŒåˆ†å˜é‡çš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ç§°ä¸ºé€»è¾‘å›å½’**ï¼Œè¿˜æœ‰å¤§é‡é€‚ç”¨äºå…¶ä»–æ•°æ®çš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚

- å›å½’ç³»æ•°çš„è§£é‡Šæ˜¯éš¾ç‚¹ï¼Œéœ€è¦é¢†åŸŸç‰¹æ®Šçš„çŸ¥è¯†ã€‚**æœ€å…³é”®çš„åœ¨äºï¼Œxæ¯å˜åŒ–ä¸€ä¸ªå•ä½yæ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œä»¥åŠæ‰€å¯¹åº”çš„betaå€¼å¦‚æœæ˜¾è‘—ï¼Œå®ƒæ‰€å¯¹åº”çš„ç‰¹å®šå«ä¹‰æ˜¯ä»€ä¹ˆã€‚**

**å…¬å¼ä¸­å„å‚æ•°çš„æ„ä¹‰**

$$
log(odds)=log(\frac{\pi}{1-\pi})=\beta_0+\beta_1X_1+...+\beta_pX_p
$$

- $\beta_0$æ˜¯æˆªè·é¡¹ï¼Œä¹Ÿç§°ä¸ºå¸¸æ•°é¡¹ï¼Œå®ƒè¡¨ç¤ºå½“æ‰€æœ‰è‡ªå˜é‡ï¼ˆ$X_1,X_2,...,X_p$ï¼‰éƒ½ä¸º0æ—¶ï¼Œlogï¼ˆoddsï¼‰çš„åŸºçº¿å€¼ã€‚æ¢å¥è¯è¯´ï¼Œ$\beta_0$è¡¨ç¤ºæ¨¡å‹åœ¨æ— ä»»ä½•é¢„æµ‹å˜é‡å½±å“æ—¶çš„logï¼ˆoddsï¼‰ã€‚

- $\beta_1,\beta_2,...,\beta_p$æ˜¯å›å½’ç³»æ•°ï¼Œåˆ†åˆ«è¡¨ç¤ºæ¯ä¸ªé¢„æµ‹å˜é‡ï¼ˆ$X_1,X_2,...,X_p$ï¼‰å¯¹logï¼ˆoddsï¼‰çš„å½±å“ã€‚å³ï¼Œæ¯ä¸ª$\beta_i$è¡¨ç¤ºå¯¹åº”è‡ªå˜é‡$X_i$å¢åŠ ä¸€ä¸ªå•ä½æ—¶ï¼Œlogï¼ˆoddsï¼‰å˜åŒ–çš„å¤§å°ã€‚

> Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge: Cambridge University Press.

ä¹Ÿå¯ä»¥å†™æˆï¼š

$$
odds=e^{\beta_0+\beta_1X_1+...+\beta_pX_p}
$$

- $\beta_0$
  - å½“ï¼ˆ$X_1,X_2,...,X_pï¼‰=0æ—¶ï¼Œodds=e^{\beta_0}ï¼Œå³e^{\beta_0}$è¡¨ç¤ºå½“æ‰€æœ‰è‡ªå˜é‡ä¸º0æ—¶ï¼Œäº‹ä»¶çš„å‘ç”Ÿæ¯”

- $\beta_1$
  - $\beta_1=log(odds_{x+1})-log(odds_x) \rightarrow e^{\beta_1}=\frac{odds_{x+1}}{odds_x}$
  - å½“å…¶ä»–è‡ªå˜é‡ä¿æŒä¸å˜æ—¶ï¼Œ$X_1$æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼ˆä»$X \rightarrow X+1$ï¼‰ï¼Œ$e^{\beta_1}$è¡¨ç¤ºäº‹ä»¶å‘ç”Ÿæ¯”çš„å€æ•°å˜åŒ–ã€‚

- è®¡ç®—

$$
log(\frac{\pi_i}{1-\pi_i})=\beta_0+\beta_1X_{i1} \rightarrow \frac{\pi_i}{1-\pi_i}=e^{\beta_0+\beta_1X_{i1}} \rightarrow \pi_i=\frac{e^{\beta_0+\beta_1X_{i1}}}{1+e^{\beta_0+\beta_1X_{i1}}}
$$

**è¡¥å……çŸ¥è¯†ï¼šæ›´å¤šå¯ç”¨çš„åˆ†å¸ƒ**

<style>
.center 
{
  width: auto;
  display: table;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="center">

|åˆ†å¸ƒç±»å‹|æè¿°æ¦‚ç‡|æè¿°å‘ç”Ÿæ¯”|é€‚ç”¨æƒ…å†µè¯´æ˜|
| :-----------: | :-----------: |:-----------: | :-----------: |
|äºŒé¡¹åˆ†å¸ƒ|æ˜¯|å¦|æè¿°næ¬¡ç‹¬ç«‹ä¼¯åŠªåˆ©è¯•éªŒä¸­æˆåŠŸæ¬¡æ•°çš„æ¦‚ç‡åˆ†å¸ƒ|
|è´å¡”åˆ†å¸ƒ|æ˜¯|æ˜¯|æè¿°ä¼¯åŠªåˆ©è¯•éªŒä¸­æˆåŠŸæ¦‚ç‡çš„å…ˆéªŒåˆ†å¸ƒï¼Œä¹Ÿå¯ä»¥ç”¨æ¥æè¿°å‘ç”Ÿæ¯”|
|å¯¹æ•°æ­£æ€åˆ†å¸ƒ|å¦|æ˜¯|æè¿°æ­£æ€åˆ†å¸ƒå˜é‡å–å¯¹æ•°åçš„åˆ†å¸ƒï¼Œå¸¸ç”¨äºæè¿°æ­£æ¯”äºå‘ç”Ÿæ¯”çš„æ•°æ®|
|æ³Šæ¾åˆ†å¸ƒ|æ˜¯|å¦|æè¿°åœ¨å›ºå®šæ—¶é—´æˆ–ç©ºé—´å†…å‘ç”ŸæŸäº‹ä»¶çš„æ¬¡æ•°çš„æ¦‚ç‡åˆ†å¸ƒ|
|ä¼½é©¬åˆ†å¸ƒ|å¦|æ˜¯|æè¿°ç­‰å¾…æ—¶é—´çš„åˆ†å¸ƒï¼Œå¸¸ç”¨äºä½œä¸ºæ³Šæ¾åˆ†å¸ƒä¸­äº‹ä»¶å‘ç”Ÿç‡çš„å…ˆéªŒåˆ†å¸ƒï¼Œå› æ­¤å¯ä»¥ç”¨æ¥æè¿°å‘ç”Ÿæ¯”|
è´ŸäºŒé¡¹åˆ†å¸ƒ|æ˜¯|å¦|æè¿°åœ¨è·å¾—ræ¬¡æˆåŠŸä¹‹å‰ç»å†næ¬¡è¯•éªŒçš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€‚ç”¨äºæˆåŠŸæ¦‚ç‡ä¸å›ºå®šçš„æƒ…å†µ|
å¤šé¡¹åˆ†å¸ƒ|æ˜¯|å¦|æè¿°å¤šé¡¹å¼è¯•éªŒä¸­å„ç§ç»“æœæ¬¡æ•°çš„æ¦‚ç‡åˆ†å¸ƒ|
Dirichletåˆ†å¸ƒ|æ˜¯|æ˜¯|æè¿°å¤šé¡¹åˆ†å¸ƒä¸­å„ç§ç»“æœæ¦‚ç‡çš„å…ˆéªŒåˆ†å¸ƒï¼Œä¹Ÿå¯ä»¥ç”¨æ¥æè¿°å‘ç”Ÿæ¯”|
</div>

### è´å¶æ–¯å¹¿ä¹‰çº¿æ€§æ¨¡å‹çš„å®šä¹‰ä¸ä»£ç å®ç°

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†é€»è¾‘å›å½’æ¨¡å‹çš„åŸºæœ¬ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å®šä¹‰æ¨¡å‹ï¼š

- æˆ‘ä»¬éœ€è¦ç¡®å®šå˜é‡ç±»å‹å’Œåˆ†å¸ƒã€‚

- éœ€è¦æ ¹æ®è¿æ¥å‡½æ•°ï¼ˆlink functionï¼‰æ¥è®¾ç½®è½¬åŒ–å‚æ•°ã€‚

- å¹¶ä¸”ä¸ºè½¬åŒ–åçš„å‚æ•°è®¾ç½®å…ˆéªŒåˆ†å¸ƒã€‚

é¦–å…ˆï¼Œä¸ºè½¬æ¢åçš„å‚æ•°è®¾ç½®å…ˆéªŒåˆ†å¸ƒï¼š

$$
data:~~Y_i|\beta_0,\beta_1~\overset{ind}{\sim}Bern(\pi_i) with \pi_i=\frac{e^{\beta_0+\beta_1X_{i1}}}{1+e^{\beta_0+\beta_1X_{i1}}}\\
priors:~~\beta_0 \sim N(0,10^2)~~;~~\beta_1 \sim N(0,10^2)
$$

**æ³¨æ„ï¼šè¿™é‡Œçš„å‚æ•°å…ˆéªŒæ˜¯ç»è¿‡ logit è½¬ååçš„å€¼ï¼Œè€Œä¸æ˜¯æ¦‚ç‡ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å…ˆéªŒé¢„æµ‹æ£€éªŒæ¥ç¡®å®šå…ˆéªŒåˆ†å¸ƒå‚æ•°è®¾ç½®æ˜¯å¦æ­£ç¡®ã€‚**

```python
# æ•°æ®å‡†å¤‡
Treatment_Coding,_ = df_clean['percentCoherence'].factorize() # é€‚ç”¨ treatment ç¼–ç 
y = df_clean['correct'].values  # ç›®æ ‡å˜é‡

# æ¨¡å‹æ„å»º
with pm.Model() as log_model1:

    # æ·»åŠ æ•°æ®ï¼Œæ–¹ä¾¿åç»­ç»˜å›¾
    pm.MutableData("percentCoherence", df_clean['percentCoherence'])

    # è®¾ç½®å…ˆéªŒ
    # é€šå¸¸æˆ‘ä»¬ä¼šä¸ºæˆªè·å’Œç³»æ•°è®¾ç½®æ­£æ€åˆ†å¸ƒçš„å…ˆéªŒ
    intercept = pm.Normal('beta_0', mu=0, sigma=10)
    coefficient = pm.Normal('beta_1', mu=0, sigma=10)
    
    # çº¿æ€§é¢„æµ‹
    linear_predictor = intercept + coefficient * Treatment_Coding
    
    # ä¼¼ç„¶å‡½æ•°
    # ä½¿ç”¨é€»è¾‘å‡½æ•°å°†çº¿æ€§é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡
    # æ–¹æ³•ä¸€ï¼šè‡ªè¡Œè¿›è¡Œ logit link è½¬æ¢
    pi = pm.Deterministic('pi', pm.math.invlogit(linear_predictor))
    likelihood = pm.Bernoulli('likelihood', p=pi, observed=y)
    # æ–¹æ³•äºŒï¼šç›´æ¥ä½¿ç”¨ logit_p è¿›è¡Œè½¬æ¢
    # likelihood = pm.Bernoulli('likelihood', logit_p=linear_predictor, observed=y)
```

æ³¨æ„ä»£ç ä¸­ä½¿ç”¨äº†`pm.math.invlogit`å‡½æ•°ï¼Œå®ƒç›¸å½“äºè®¡ç®—äº† Logistic sigmoid functionï¼Œå³$1/(1+e^{-\mu})$

ä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬è‡ªè¡Œè¿›è¡Œè½¬åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªè¡Œå®šä¹‰è¿™ä¸ªå‡½æ•°ï¼Œå¦‚ä¸‹ï¼š

```python
def inv_logit(x):
    return np.exp(x) / (1 + np.exp(x))
```

**å…ˆéªŒé¢„æµ‹æ£€éªŒ**

ä½¿ç”¨`pm.sample_prior_predictive`è¿›è¡Œå…ˆéªŒé¢„æµ‹æ£€éªŒï¼Œæ¥æŸ¥çœ‹ç”±å½“å‰å…ˆéªŒç»„åˆç”Ÿæˆçš„$\pi$æ˜¯å¦éƒ½åœ¨0è‡³1çš„èŒƒå›´å†…

```python
log1_prior = pm.sample_prior_predictive(samples=50, 
                                          model=log_model1,
                                          random_seed=84735)
log1_prior
```

- åœ¨æ¨¡å‹å®šä¹‰ä¸­æˆ‘ä»¬å·²ç»å¯¹piè¿›è¡Œå®šä¹‰ï¼Œå› æ­¤`pm.sample_prior_predictive`å°±ä¼šè‡ªåŠ¨ç”Ÿæˆå¯¹piçš„é¢„æµ‹

- è¯¥é¢„æµ‹å‚¨å­˜åœ¨priorä¸­

- æˆ‘ä»¬è®¾ç½®æŠ½æ ·æ•°ä¸º50ï¼Œè¿™ä½“ç°åœ¨ç»´åº¦drawä¸­

- ç»“åˆå¾ªç¯ï¼Œä½¿ç”¨`sns.lineplot`ç»˜åˆ¶å‡ºæ¯ä¸ªå›é¿åˆ†æ•°å¯¹åº”çš„Ï€å€¼å¹¶è¿æ¥æˆå…‰æ»‘çš„æ›²çº¿

```python
#å¯¹äºä¸€æ¬¡æŠ½æ ·ï¼Œå¯ä»¥ç»˜åˆ¶å‡ºä¸€æ¡æ›²çº¿ï¼Œç»“åˆå¾ªç¯ç»˜åˆ¶å‡º50æ¡æ›²çº¿
for i in range(log1_prior.prior.dims["draw"]):
    sns.lineplot(x = log1_prior.constant_data["percentCoherence"],
                y = log1_prior.prior["pi"].stack(sample=("chain", "draw"))[:,i], c="grey" )

#è®¾ç½®xã€yè½´æ ‡é¢˜å’Œæ€»æ ‡é¢˜    
plt.xlabel("percentCoherence",
           fontsize=12)
plt.ylabel("probability of correct",
           fontsize=12)
plt.suptitle("Relationships between percentCoherence and the probability of correct",
           fontsize=14)
sns.despine()
plt.show()
```

![alt text](image-2.png)

- æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ä¸€è‡´æ€§10%å’Œ40%è¿™ä¸¤ç§æ¡ä»¶ä¸‹ï¼Œæ‰€å¯¹åº”çš„æ­£ç¡®ç‡åŸºæœ¬ä¸Šéƒ½åœ¨0-1èŒƒå›´ä¹‹é—´ï¼Œå› æ­¤æ˜¯å¯ä»¥æ¥å—ã€‚å½“ç„¶ï¼Œä¹Ÿå¯ä»¥å¯¹priorè¿›è¡Œä¼˜åŒ–ï¼Œä½†éœ€è¦æ¯”è¾ƒè°¨æ…åœ°å»è€ƒè™‘ã€‚

***æ³¨æ„ï¼špriorçš„ä¼˜åŒ–éœ€è¦å¾ˆè°¨æ…åœ°è¿›è¡Œ***

- åœ¨è´å¶æ–¯ä¸­ï¼Œå¦‚æœä¸ºäº†å¾—åˆ°ä¸€ä¸ªç‰¹å®šçš„åéªŒå»æŠŠå…ˆéªŒæ”¹çš„ç‰¹åˆ«ç²¾ç¡®ï¼Œæœ€åå¯¼è‡´æ•°æ®çš„ä½œç”¨å¾ˆå°ï¼Œè¿™å®é™…ä¸Šç±»ä¼¼äºæˆ‘ä»¬ä¼ ç»Ÿé›¶å‡è®¾æ£€éªŒä¸­çš„â€œP hackingâ€ï¼Œä¹Ÿå°±æ˜¯ä¸ºäº†è®©På€¼å˜å°åšå„ç§å„æ ·æ•°æ®çš„ä¼˜åŒ–ã€‚å› æ­¤ï¼Œä¸ºäº†é¿å…è¿™ç§å«Œç–‘ï¼Œå…ˆéªŒä¼˜åŒ–æ˜¯éœ€è¦éå¸¸è°¨æ…çš„ã€‚


**MCMCé‡‡æ · & æ¨¡å‹è¯Šæ–­**

```python
#===========================
#     æ³¨æ„ï¼ï¼ï¼ä»¥ä¸‹ä»£ç å¯èƒ½éœ€è¦è¿è¡Œ35s~1åˆ†é’Ÿå·¦å³
#===========================
with log_model1:
    # æ¨¡å‹ç¼–è¯‘å’Œé‡‡æ ·
    log_model1_trace = pm.sample(draws=5000,                 
                                tune=1000,                  
                                chains=4,                     
                                discard_tuned_samples=True, 
                                random_seed=84735)
```

```python
az.plot_trace(log_model1_trace,
              var_names=["beta_0","beta_1"],
              figsize=(7, 6),
              compact=False)
plt.show()
```

![alt text](image-6.png)

**åéªŒå‚æ•°è§£é‡Š**

```python
fitted_parameters = az.summary(log_model1_trace, var_names=["beta_0","beta_1"])
fitted_parameters
```

![alt text](image-3.png)

ä¸ºäº†å°†å‘ç”Ÿæ¯”è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—é€‚ç”¨é€†è¿ç®—ï¼Œå¦‚ä¸‹ï¼š

```python
def inv_logit(log_odds):
    return np.exp(log_odds) / (1 + np.exp(log_odds))

p_coh10 = inv_logit( fitted_parameters.loc["beta_0", "mean"])
p_coh40 = inv_logit( 
    fitted_parameters.loc["beta_1", "mean"] + \
        fitted_parameters.loc["beta_1", "mean"]
)

print(f"p(coherence=10) = {p_coh10:.3f}", f"p(coherence=40) = {p_coh40:.3f}")
```
p(coherence=10) = 0.687 p(coherence=40) = 0.972

```python
# é€šè¿‡ inv_logit å°† beta å‚æ•°è¿›è¡Œè½¬æ¢
az.plot_posterior(log_model1_trace, var_names=["beta_0"], transform = inv_logit)
plt.show()
```

![alt text](image-4.png)

ç»“æœæ˜¾ç¤ºï¼š

- $\beta_0=0.786ï¼Œé‚£ä¹ˆe^{\beta_0}=2.195ï¼ŒX_{i1}=0æ—¶ï¼Œåˆ™äº‹ä»¶å‘ç”Ÿçš„oddsä¸ºe^{\beta_0}ä¸º2.195$ã€‚

- $\beta_1=1.765ï¼Œe^{\beta_1}=5.859ï¼ŒX_{i1}æ¯å¢åŠ 1ä¸ªå•ä½ï¼Œoddså°†å¢åŠ e^{\beta_1}çš„5.859å€$ã€‚

- ç„¶è€Œï¼Œ$\beta_1$çš„4%HDIåŒ…æ‹¬0ï¼Œè¯´æ˜ç‚¹çš„ä¸€è‡´æ€§æ–¹å‘æ¦‚ç‡ä¸èƒ½æœ‰æ•ˆé¢„æµ‹åˆ¤æ–­æ˜¯å¦æ­£ç¡®çš„æ¦‚ç‡ã€‚

**åéªŒå›å½’æ¨¡å‹**

**ç»˜åˆ¶åéªŒé¢„æµ‹å›å½’çº¿**

- å’Œå…ˆéªŒé¢„æµ‹æ¨¡å‹ç±»ä¼¼çš„ï¼Œé€šè¿‡MCMCé‡‡æ ·ï¼Œä¹ŸåŒæ ·ç”Ÿæˆäº†å¯¹Ï€çš„ä¼°è®¡ï¼Œå‚¨å­˜åœ¨posteriorä¸­ã€‚

- æœ‰4æ¡é©¬å°”ç§‘å¤«é“¾ï¼Œæ¯æ¡é“¾ä¸Šçš„é‡‡æ ·æ•°ä¸º2000ï¼Œæ‰€ä»¥å¯¹äºæ¯ä¸€ä¸ªxï¼Œéƒ½ç”Ÿæˆäº†20000ä¸ªé¢„æµ‹å€¼Ï€ï¼Œè¿™æ ·å°±å¯¹åº”ç€20000æ¡åéªŒé¢„æµ‹å›å½’çº¿

- è¿™é‡Œæˆ‘ä»¬åªéœ€è¦ç”»å‡º100æ¡å³å¯

```python
log_model1_trace
```

```python
#å¯¹äºä¸€æ¬¡æŠ½æ ·ï¼Œå¯ä»¥ç»˜åˆ¶å‡ºä¸€æ¡æ›²çº¿ï¼Œç»“åˆå¾ªç¯ç»˜åˆ¶å‡º50æ¡æ›²çº¿
ys = log_model1_trace.posterior["pi"].stack(sample=("chain", "draw"))
for i in range(100):
    sns.lineplot(x = log_model1_trace.constant_data["percentCoherence"],
            y = ys[:,i], 
            c="grey",
            alpha=0.4)
    
#è®¾ç½®xã€yè½´æ ‡é¢˜å’Œæ€»æ ‡é¢˜    
plt.xlabel("percentCoherence",
           fontsize=12)
plt.ylabel("probability of correct",
           fontsize=12)
plt.suptitle("100 posterior plausible models",
           fontsize=14)
sns.despine()
plt.show()
```

![alt text](image-5.png)

**å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹&åˆ†ç±»**

- é™¤äº†å¯¹å½“å‰æ•°æ®ç»“æœåšå‡ºè§£é‡Šï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å½“å‰çš„å‚æ•°é¢„æµ‹å€¼ï¼Œå¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹

- ç°åœ¨å‡è®¾æœ‰ä¸€æ‰¹æ–°æ•°æ®ï¼Œé‚£ä¹ˆè¢«è¯•åœ¨â€œpercentCoherenceâ€ä¸º 40çš„æƒ…å†µä¸‹ï¼Œå¯¹æ–°æ•°æ®è¿›è¡Œæ­£ç¡®åˆ¤æ–­çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ

- å³å½“$X_i=40$æ—¶ï¼Œå¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹å’Œåˆ†ç±»

- ç”±äºä¾‹å­ä¸­çš„è‡ªå˜é‡ä½¿ç”¨äº†treatment codingçš„ç¼–ç æ–¹å¼ï¼Œæ‰€ä»¥$X_i=40$å¯¹åº”ä¸º$X_i=1$

$$
Y|\beta_0,\beta_1 \sim Bern(\pi)~with~log(\frac{\pi}{1-\pi})=\beta_0+\beta_1*1
$$

```python
odds = log_model1_trace.posterior["beta_0"]+ log_model1_trace.posterior["beta_1"] * 1
pi = inv_logit(odds)
Y_hat = np.random.binomial(n=1, p=pi)[0]

# ç»Ÿè®¡å…¶ä¸­0å’Œ1çš„ä¸ªæ•°ï¼Œå¹¶é™¤ä»¥æ€»æ•°ï¼Œå¾—åˆ°0å’Œ1å¯¹åº”çš„æ¯”ä¾‹å€¼
y_pred_freq = np.bincount(Y_hat)/len(Y_hat)

# ç»˜åˆ¶æŸ±çŠ¶å›¾
bars = plt.bar([0, 1], y_pred_freq, color="#70AD47")

# ç”¨äºåœ¨æŸ±çŠ¶å›¾ä¸Šæ ‡æ˜æ¯”ä¾‹å€¼
for bar, freq in zip(bars, y_pred_freq):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f"{freq:.2f}", ha='center', va='bottom')

#å¯¹åˆ»åº¦ã€æ ‡é¢˜ã€åæ ‡è½´æ ‡é¢˜è¿›è¡Œè®¾ç½®
plt.xticks([0, 1])
plt.suptitle("Out-of-sample prediction(X=1)")
plt.xlabel("correct")
plt.ylabel("proportion")
sns.despine()
```

![alt text](image-7.png)

**è¯„ä¼°åˆ†ç±»ç»“æœ**

- æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**æ··æ·†çŸ©é˜µ(confusion matrix)**æ¥å¯¹**çœŸå®ç»“æœ**ä¸**é¢„æµ‹ç»“æœ**è¿›è¡Œæ¯”è¾ƒå’Œè¯„ä¼°(0ä¸ºé˜´æ€§ï¼Œ1ä¸ºé˜³æ€§)ï¼š

è¿™ç§çŸ©é˜µç±»ä¼¼äºæˆ‘ä»¬çš„ä¿¡å·æ£€æµ‹è®ºã€ä¸€ç±»é”™è¯¯äºŒç±»é”™è¯¯çŸ©é˜µç­‰ç­‰ï¼Œè™½ç„¶å¥¹ä»¬æœ‰ä¸åŒçš„æœ¯è¯­ï¼Œä½†æ˜¯åŸºæœ¬åŸç†æ˜¯ç›¸åŒçš„ã€‚

  - a: çœŸé˜´æ€§ï¼ˆTrue Negativeï¼ŒTNï¼‰è¡¨ç¤ºè¢«æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿä¾‹çš„æ ·æœ¬æ•°

  - b: å‡é˜³æ€§ï¼ˆFalse Positiveï¼ŒFPï¼‰è¡¨ç¤ºè¢«é”™è¯¯é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ•°

  - c: å‡é˜´æ€§ï¼ˆFalse Negativeï¼ŒFNï¼‰è¡¨ç¤ºè¢«é”™è¯¯é¢„æµ‹ä¸ºè´Ÿä¾‹çš„æ ·æœ¬æ•°

  - d: çœŸé˜³æ€§ï¼ˆTrue Positiveï¼ŒTPï¼‰è¡¨ç¤ºè¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ•°

<style>
.center 
{
  width: auto;
  display: table;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="center">

||$\hat{Y}=0$|$\hat{Y}=1$|
| :-----------: | :-----------: | :-----------: |
|$Y=0$|a|b|
|$Y=1$|c|d|
</div>

åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå‡†ç¡®æ€§ï¼ˆAccuracyï¼‰ã€æ•æ„Ÿæ€§ï¼ˆSensitivityï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆSpecificityï¼‰æ˜¯å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥é€šè¿‡åœ¨å¾—åˆ° a b c d çš„æ•°é‡ä¹‹åè¿›è¡Œè®¡ç®—ï¼š

1. **å‡†ç¡®æ€§(accuracy)** ï¼šå‡†ç¡®æ€§æ˜¯æŒ‡åˆ†ç±»æ¨¡å‹æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ã€‚

- å‡†ç¡®æ€§è¡¡é‡äº†æ¨¡å‹æ€»ä½“çš„åˆ†ç±»æ­£ç¡®ç‡ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½è¶Šå¥½ã€‚

$$
accuracy=\frac{(TP+TN)}{(TP+TN+FP+FN)}=\frac{a+d}{a+b+d+d}
$$

2. **æ•æ„Ÿæ€§(sensitivity)** ï¼šæ•æ„Ÿæ€§ä¹Ÿç§°ä¸ºå¬å›ç‡ï¼ˆRecallï¼‰ï¼Œå®ƒæ˜¯æŒ‡åœ¨æ‰€æœ‰å®é™…ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚

- æ•æ„Ÿæ€§è¡¡é‡äº†æ¨¡å‹å¯¹äºæ­£ä¾‹çš„è¯†åˆ«èƒ½åŠ›ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºæ¨¡å‹å¯¹äºæ­£ä¾‹çš„é¢„æµ‹èƒ½åŠ›è¶Šå¥½ã€‚

$$
sensitivity=\frac{TP}{(TP+FN)}=\frac{d}{c+d}
$$

3.**ç‰¹å¼‚æ€§(specificity)** ï¼šç‰¹å¼‚æ€§æ˜¯æŒ‡åœ¨æ‰€æœ‰å®é™…ä¸ºè´Ÿä¾‹çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿä¾‹çš„æ¯”ä¾‹ã€‚

- ç‰¹å¼‚æ€§è¡¡é‡äº†æ¨¡å‹å¯¹äºè´Ÿä¾‹çš„è¯†åˆ«èƒ½åŠ›ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºæ¨¡å‹å¯¹äºè´Ÿä¾‹çš„é¢„æµ‹èƒ½åŠ›è¶Šå¥½ã€‚

$$
specificity=\frac{TN}{(TN+FP)}=\frac{a}{a+b}
$$

ç°åœ¨ï¼Œæˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹è¿™ä¸‰ä¸ªæŒ‡æ ‡ï¼š

```python
ys = log_model1_trace.posterior["pi"].stack(sample=("chain", "draw"))
df_clean["pi"] = ys.mean(dim="sample").values

predictions = []
for i in df_clean["pi"]:
    prediction = np.random.binomial(n=1, p=i)
    predictions.append(prediction)

df_clean["prediction"] = predictions
df_clean
```

```python
def calculate_contingency_table(df, y="correct", yhat="prediction"):
    
    # è®¡ç®—å„ç§æƒ…å†µçš„æ•°é‡
    TN = ((df[y] == 0) & (df[yhat] == 0)).sum()  # çœŸé˜´æ€§
    FP = ((df[y] == 0) & (df[yhat] == 1)).sum()  # å‡é˜³æ€§
    FN = ((df[y] == 1) & (df[yhat] == 0)).sum()  # å‡é˜´æ€§
    TP = ((df[y] == 1) & (df[yhat] == 1)).sum()  # çœŸé˜³æ€§
    
    # åˆ›å»ºä¸€ä¸ªDataFrameæ¥è¡¨ç¤ºåˆ—è”è¡¨
    contingency_df = pd.DataFrame({
        '$\\hat{Y} = 0$': [TN, FN],
        '$\\hat{Y} = 1$': [FP, TP]
    }, index=['$Y=0$', '$Y=1$'])
    
    return (TN, FP, TN, FN), contingency_df

# è®¡ç®—ä¸¤ä¸ª percentCoherence å€¼ä¸‹çš„åˆ—è”è¡¨
(true_positive, false_positive, true_negative, false_negative), contingency_table = calculate_contingency_table(df_clean)

contingency_table
```

<style>
.center 
{
  width: auto;
  display: table;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="center">

||$\hat{Y}=0$|$\hat{Y}=1$|
| :-----------: | :-----------: | :-----------: |
|$Y=0$|30|72|
|$Y=1$|69|372|
</div>

```python
# å®šä¹‰è®¡ç®—æŒ‡æ ‡å‡½æ•°
def calculate_metrics(TP, FP, TN, FN):
    # è®¡ç®—å‡†ç¡®æ€§
    accuracy = (TP + TN) / (TP + TN + FP + FN)

    # è®¡ç®—æ•æ„Ÿæ€§
    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0

    # è®¡ç®—ç‰¹å¼‚æ€§
    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0

    return accuracy, sensitivity, specificity

# è®¡ç®—æŒ‡æ ‡
accuracy, sensitivity, specificity = calculate_metrics(true_positive, false_positive, true_negative, false_negative)

# æ‰“å°ç»“æœ
print(f"True Positive: {true_positive}")
print(f"False Positive: {false_positive}")
print(f"True Negative: {true_negative}")
print(f"False Negative: {false_negative}")
print(f"å‡†ç¡®æ€§: {accuracy}")
print(f"æ•æ„Ÿæ€§: {sensitivity}")
print(f"ç‰¹å¼‚æ€§: {specificity}")
```

> è¾“å‡ºç»“æœï¼š
True Positive: 30
False Positive: 72
True Negative: 30
False Negative: 69
å‡†ç¡®æ€§: 0.29850746268656714
æ•æ„Ÿæ€§: 0.30303030303030304
ç‰¹å¼‚æ€§: 0.29411764705882354

å¯ä»¥çœ‹å‡ºï¼Œè¿™é‡Œçš„å‡†ç¡®ç‡å’Œæ•æ„Ÿæ€§éƒ½ä¸æ˜¯å¾ˆé«˜ã€‚ç”±äºåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­æˆ‘ä»¬æ˜¯ä½¿ç”¨æ¯ä¸€ç§æ¡ä»¶ä¸‹åéªŒåˆ†å¸ƒçš„å‡å€¼è¿›è¡Œè®¡ç®—ï¼Œå¹¶æœªè€ƒè™‘æ•´ä¸ªåéªŒåˆ†å¸ƒã€‚å¤§å®¶å¯ä»¥æ€è€ƒä¸€ä¸‹ï¼Œå¦‚æœè€ƒè™‘æ‰€æœ‰æ•´ä¸ªåéªŒåˆ†å¸ƒåï¼Œé¢„æµ‹çš„ç»“æœæ˜¯å¦ä¼šå‘ç”Ÿå˜åŒ–ï¼Ÿ

***å¦‚æœæ–°æ•°æ®çš„è‡ªå˜é‡æ˜¯ä¸€ä¸ªæ–°çš„å–å€¼å‘¢ï¼Ÿ***

å½“å‰æˆ‘ä»¬åªè€ƒè™‘äº†percentCoherenceä¸º10æˆ–40ã€‚æˆ‘ä»¬è¿˜å¯ä»¥è¿›ä¸€æ­¥æ¢ç©¶å½“percentCoherenceä¸º25æ—¶çš„åˆ¤æ–­æ­£ç¡®ç‡ï¼Œå³$X_i=25$ã€‚

ğŸ””æ³¨æ„ï¼š

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è‡ªå˜é‡percentCoherenceæ˜¯**è¿ç»­å˜é‡**ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨ç¼–ç çš„æ—¶å€™ä»ç„¶æŒ‰ç…§**ç¦»æ•£å˜é‡**çš„æ ‡å‡†è¿›è¡Œè®¾å®šï¼Œå³æŒ‰ç…§treatment coding çš„æ–¹å¼è¿›è¡Œ0å’Œ1çš„ç¼–ç ã€‚

- **å› æ­¤ï¼Œè¿™é‡Œå¯èƒ½å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å¤„ç†è¿ç»­å˜é‡çš„æ—¶å€™å®é™…ä¸Šä¸éœ€è¦åšè¿™ç§ç¼–ç ï¼Œè€Œæ˜¯ç›´æ¥å¸¦å…¥è‡ªå˜é‡å–å€¼å³å¯ã€‚**

- ä¾‹å¦‚ï¼Œå½“æ–°æ•°æ®çš„è‡ªå˜é‡å–å€¼ä¸º$X_i=25$æ—¶ï¼Œæˆ‘ä»¬ç›´æ¥å°†å…¶å¸¦å…¥è¿›æ–¹ç¨‹å³å¯ï¼Œè€Œä¸éœ€è¦å°†å…¶æŒ‰ç…§ç¦»æ•£å˜é‡ç¼–ç ä¸º0.5ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ–°å¸¦å…¥çš„æ•°æ®éœ€è¦å’Œæœ€å¼€å§‹å»ºæ¨¡æ•°æ®çš„å•ä½ä¿æŒä¸€è‡´ã€‚

### è¡¥å……ï¼šä½¿ç”¨bambiå»ºç«‹logisticå›å½’æ¨¡å‹

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨bambi æä¾›çš„é»˜è®¤å…ˆéªŒæ¥æ„å»ºæ¨¡å‹ã€‚ å¯ä»¥çœ‹åˆ°ï¼š

- å…ˆéªŒä¸ºï¼š

Intercept ~ Normal(mu: 0.0, sigma: 3.6269)

C(percentCoherence) ~ Normal(mu: 0.0, sigma: 5.0062)

- æ¨¡å‹åˆ†å¸ƒä¸ºï¼šbernoulli

- é“¾æ¥å‡½æ•°ä¸ºï¼šp = logit

- C(percentCoherence) ä»£è¡¨å°† percentCoherence å˜é‡ç¼–ç ä¸ºåˆ†ç±»å˜é‡ (categorical variable)ã€‚

```python
def inv_logit(log_odds):
    return np.exp(log_odds) / (1 + np.exp(log_odds))
```

```python
import bambi as bmb

bambi_logit = bmb.Model("correct ~ C(percentCoherence)", df_clean, family="bernoulli")
bambi_logit
```

**æ¨¡å‹æ‹Ÿåˆ**

è¿™é‡Œä½¿ç”¨ bambi æä¾›çš„é»˜è®¤æ‹Ÿåˆè®¾ç½®ã€‚

- åŒ…æ‹¬ 4 æ¡ MCMC é“¾ï¼Œæ¯ä¸ªé“¾ 2000 ä¸ªè¿­ä»£ï¼Œå…¶ä¸­å‰ 1000 ä¸ªä¸º burn-in é˜¶æ®µã€‚

```python
model_fitted = bambi_logit.fit(random_seed=84735)
model_fitted
```

```python
fitted_parameters = az.summary(model_fitted)
fitted_parameters
```

![alt text](image-10.png)

ç”±æ­¤å¯è§ï¼Œä½¿ç”¨ bambi å»ºç«‹çš„ logistic å›å½’æ¨¡å‹å¾—åˆ°çš„ç»“æœä¸æˆ‘ä»¬ä½¿ç”¨ PyMC å»ºç«‹çš„æ¨¡å‹ç»“æœå‡ ä¹ä¸€è‡´ï¼š

- C(percentCoherence)[40] çš„å‡å€¼ä¸º 1.762ï¼Œä¸ beta_1 çš„å‡å€¼ 1.766 éå¸¸æ¥è¿‘ã€‚

- Intercept çš„å‡å€¼ä¸º 0.785ï¼Œä¸ beta_0 çš„å‡å€¼ 0.784 å·®å¼‚å¾®å°ã€‚


```python
p_coh10 = inv_logit( fitted_parameters.loc["Intercept", "mean"])
p_coh40 = inv_logit( 
    fitted_parameters.loc["Intercept", "mean"] + \
        fitted_parameters.loc["C(percentCoherence)[40]", "mean"]
)

print(f"p(coherence=10) = {p_coh10:.3f}", f"p(coherence=40) = {p_coh40:.3f}")
```

```python
posterior_predictive = bambi_logit.predict(model_fitted, kind="pps")
model_fitted
```

```python
az.plot_ppc(model_fitted, num_pp_samples=50)
sns.despine()
```

![alt text](image-11.png)


**æ€»ç»“**

æœ¬èŠ‚è¯¾å­¦ä¹ äº†å¦‚ä½•é€šè¿‡å¹¿ä¹‰çº¿æ€§æ¨¡å‹(Generalized linear model, GLM)æ‹ŸåˆäºŒå…ƒå†³ç­–å˜é‡ã€‚

é‡ç‚¹åœ¨äºï¼š

- äº†è§£äºŒå…ƒå†³ç­–å˜é‡é€‚åˆçš„åˆ†å¸ƒï¼Œä¼¯åŠªåˆ©(Bernoulli)åˆ†å¸ƒã€‚

- äº†è§£å¦‚ä½•é€šè¿‡æ¦‚ç‡ã€å‘ç”Ÿç‡å’Œé“¾æ¥å‡½æ•°(link function)æ¥è¡¨ç¤ºçº¿æ€§æ¨¡å‹ã€‚

- å­¦ä¹ æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®æ€§ï¼ˆAccuracyï¼‰ã€æ•æ„Ÿæ€§ï¼ˆSensitivityï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆSpecificityï¼‰ã€‚

![alt text](image-16.png)