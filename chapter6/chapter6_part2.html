

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>MCMC诊断 &#8212; Bayesian Inference with Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter6/chapter6_part2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture8 : A Simple Linear Regression Model with PyMC" href="../chapter7/chapter7_part1.html" />
    <link rel="prev" title="Lecture7 : Posterior Inference &amp; Estimation" href="chapter6_part1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../chapter_overview/intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Bayesian Inference with Python - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Bayesian Inference with Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../chapter_overview/intro.html">
                    序言
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">0. 课程概述</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_overview/Part1.html">为什么要学习贝叶斯推断?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_overview/Part2.html">贝叶斯推断的三个例子</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_overview/Part3.html">本课程的主要内容与形式</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">1. 初识贝叶斯法则(Bayes'Rule)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter1_Bayesrule/Bayesrule_part1.html">1.0 本课程演示平台: 和鲸平台</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1_Bayesrule/Bayesrule_part2.html">1.1 单一事件的贝叶斯模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1_Bayesrule/Bayesrule_part3.html">1.2 随机变量的贝叶斯模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1_Bayesrule/Bayesrule_part4.html">1.3 贝叶斯与频率主义的简单对比</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">2. 一个真正的贝叶斯模型——Beta-Binomial Model</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter2_Beta_Binomial/BetaBinomial_part1.html">2.0 先验分布: Beta分布</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2_Beta_Binomial/BetaBinomial_part2.html">2.1 似然函数: Binomial分布</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2_Beta_Binomial/BetaBinomial_part3.html">2.2 后验分布</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2_Beta_Binomial/BetaBinomial_part4.html">2.3 通过模拟可视化来观察模型</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3. Balance and Sequentiality in Bayesian Analyses</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter3/Lecture4_part1.html">3.0 回顾Bayes'Rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/Lecture4_part2.html">3.1 不同数据对后验的影响</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/Lecture4_part3.html">3.2 不同似然对后验的影响</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/Lecture4_part4.html">3.3 不同先验对后验的影响</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">4. Bayesian Inference:From Traditional Foundations to Current Practices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter4/chapter4_part1.html">4.0 内容回顾</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter4/chapter4_part2.html">4.1 Grid approximation 网格近似</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">5. Markov Chain Monte Carlo(MCMC)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter5/chapter5_part1.html">5.0 Monte Carlo &amp; Markov Chain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter5/chapter5_part2.html">5.1 Metropolis-Hastings(MH)算法</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">6. Posterior Inference &amp; Estimation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6_part1.html">6.0 A Beta-Binomial example in pymc</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6.1 MCMC诊断</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">7. A Simple Linear Regression Model with PyMC</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter7/chapter7_part1.html">7.0 贝叶斯一般线性回归模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter7/chapter7_part2.html">7.1 先验预测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter7/chapter7_part3.html">7.2 后验预测</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/hcp4715/PyBayesianBook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/hcp4715/PyBayesianBook/issues/new?title=Issue%20on%20page%20%2Fchapter6/chapter6_part2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter6/chapter6_part2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>MCMC诊断</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ppl-probabilistic-programming-language">什么是概率编程语言（PPL，probabilistic programming language）</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-diagnostics">对马尔科夫链的诊断 (Markov chain diagnostics)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trace-plots">轨迹图（trace plots）</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ess-effective-sample-size-autocorrelation">有效样本大小 ESS(effective sample size)和自相关性 (autocorrelation)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fast-vs-slow-mixing-markov-chains">重要概念：Fast vs slow mixing Markov chains</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effective-sample-size-ratio">有效样本量比率 Effective sample size ratio</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-hat-and-comparing-parallel-chains">R-hat and comparing parallel chains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">代码演示–MCMC诊断</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-estimation">Posterior estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#credible-intervals">可信区间(credible intervals)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-highest-density-interval-hdi">最大后验概率密度区间(the highest density interval, HDI)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="mcmc">
<h1>MCMC诊断<a class="headerlink" href="#mcmc" title="Permalink to this heading">#</a></h1>
<p>在上一课中，我们已经介绍了PyMC的基本用法。</p>
<ul class="simple">
<li><p>其实，PyMC 就是一种 概率编程语言（PPL）框架，它帮助我们在 Python 中进行贝叶斯模型的构建和后验推断。</p></li>
<li><p>通过 PyMC，我们可以定义先验分布、设置似然函数、运行采样算法，并在采样后对模型结果进行可视化和诊断。</p></li>
<li><p>PyMC 支持多种采样方法，如 MCMC（马尔可夫链蒙特卡洛），这使得 PyMC 在保持高效性的同时，能够处理更复杂的模型，满足多种统计建模需求。</p></li>
<li><p>PyMC 只是 PPL 框架的一个代表，不同的 PPL 框架（如 Stan）在设计和功能上各有特色。</p></li>
</ul>
<section id="ppl-probabilistic-programming-language">
<h2>什么是概率编程语言（PPL，probabilistic programming language）<a class="headerlink" href="#ppl-probabilistic-programming-language" title="Permalink to this heading">#</a></h2>
<p>概率编程语言的关键在于**自动进行概率分布（尤其是贝叶斯后验分布）**的推理过程。</p>
<ul class="simple">
<li><p>传统贝叶斯后验分布的推理过程中，我们需要定义先验，确定似然函数，然后使用 MCMC 等方法来计算后验分布：</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="n">Y</span> <span class="o">=</span> <span class="mi">9</span>    
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>    
<span class="n">pi</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>    
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>    

<span class="n">posterior_unstandardized</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">pi</span>    
<span class="c1"># then we need MCMC to get the posterior distribution    </span>
<span class="c1"># each iteration we will calculate posterior_unstandardized = likelihood * pi    </span>
<span class="n">mh_simulation</span> <span class="o">=</span> <span class="n">mh_tour</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>对于概率编程语言，我们只需要定义先验和数据模型，然后就可以自动推断出后验分布:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="n">Y</span> <span class="o">=</span> <span class="mi">9</span>    
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">bb_model</span><span class="p">:</span>    
    <span class="n">pi</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;pi&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>    
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pi</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<p>PPL的优势在于：我们只需要定义先验和似然，在无需考虑如何使用MCMC等方法的前提下自动计算后验分布</p>
<p>PPL 框架还提供了其他更多的功能和特点，我们将在之后的课程中慢慢接触到：</p>
<style>
.center 
{
  width: auto;
  display: table;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="center">
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>功能</p></th>
<th class="head text-center"><p>描述</p></th>
<th class="head text-center"><p>主要用途</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><strong>模型定义</strong></p></td>
<td class="text-center"><p>用简洁语法定义先验分布和似然函数，支持分层模型</p></td>
<td class="text-center"><p>简化模型构建流程，灵活定义复杂结构</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>自动采样</strong></p></td>
<td class="text-center"><p>提供 MCMC (HMC、NUTS)、变分推断等多种近似算法</p></td>
<td class="text-center"><p>高效获取后验样本，适应高维复杂模型</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>自动微分</strong></p></td>
<td class="text-center"><p>自动计算梯度，支持高维复杂模型</p></td>
<td class="text-center"><p>加速模型优化，适应复杂计算需求</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>诊断与调试</strong></p></td>
<td class="text-center"><p>提供 Trace Plot、R-hat 等诊断工具，检测采样质量与收敛性</p></td>
<td class="text-center"><p>确保采样结果可靠，识别采样问题</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>后验分析和可视化</strong></p></td>
<td class="text-center"><p>提供后验分布图、密度图、置信区间等可视化工具</p></td>
<td class="text-center"><p>直观理解参数分布和不确定性，辅助决策</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>后验预测</strong></p></td>
<td class="text-center"><p>从后验分布生成新观测数据，用于模型验证</p></td>
<td class="text-center"><p>检查模型合理性，评估预测效果</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>灵活建模扩展性</strong></p></td>
<td class="text-center"><p>支持模块化建模、集成外部工具，构建自定义模型</p></td>
<td class="text-center"><p>适应不同应用场景，增强模型的扩展性</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>贝叶斯概率编程中的核心术语</strong></p>
<p>这些术语在贝叶斯统计和概率编程中具有重要应用。除了已经讲过的部分概念外，本节课将涉及一些新概念：</p>
<style>
.center 
{
  width: auto;
  display: table;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="center">
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>术语（中英文全称）</p></th>
<th class="head text-center"><p>定义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><strong>先验分布（Prior Distribution）</strong></p></td>
<td class="text-center"><p>先验分布是指在观察数据之前，研究人员对模型中未知参数的初始信念。这种信念可以基于现有研究或试点数据形成。</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>似然函数（Likelihood Function）</strong></p></td>
<td class="text-center"><p>似然函数是指特定参数下观测数据的概率，它是统计模型中选择参数的概率函数。例如，伯努利函数是描述硬币投掷统计的似然函数。</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>后验分布（Posterior Distribution）</strong></p></td>
<td class="text-center"><p>后验分布是指在观察到数据后，根据贝叶斯规则平衡先验知识与观测数据，对参数的更新信念。</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>马尔可夫链蒙特卡洛方法（Markov Chain Monte Carlo, MCMC）</strong></p></td>
<td class="text-center"><p>MCMC是一种通过模拟来推断后验分布的采样方法。通过算法构建多个马尔可夫链，使得它们的平稳分布与感兴趣的后验分布相匹配，这个过程称为MCMC收敛。</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>有效样本大小（Effective Sample Size, ESS）</strong></p></td>
<td class="text-center"><p>有效样本大小是指与N个自相关样本具有相同估计力的独立样本数量。ESS常用于判断MCMC链中的抽样数量是否足以保证可靠的估计不确定性。</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>Gelman-Rubin统计量（Gelman-Rubin Statistics, R̂）</strong></p></td>
<td class="text-center"><p>Gelman-Rubin统计量是链内变异性与链间变异性的比率。所有参数和感兴趣量接近1.0的值表明马尔可夫链蒙特卡洛算法已足够收敛到平稳分布。</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>最高密度区间（Highest Density Interval, HDI）</strong></p></td>
<td class="text-center"><p>最高密度区间是贝叶斯统计中参数可信范围的估计。它包括后验分布中的一个区间，该区间内的每个点的密度都高于区间外的点。</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>实践等效区间（Region of Practical Equivalence, ROPE）</strong></p></td>
<td class="text-center"><p>实践等效区间是预先定义的参数值范围，这些值被认为在实践上等同于零。用于判断参数估计是否显著不同于零。</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>贝叶斯因子（Bayes Factor, BF）</strong></p></td>
<td class="text-center"><p>贝叶斯因子量化了一个统计模型相对于另一个模型的证据强度。大于1的值表明相对于原始模型，替代模型得到了更多的支持。</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>采样样本（Samples/Draws）</strong></p></td>
<td class="text-center"><p>采样样本值是指从后验分布中抽取的参数值，用作后验分布的近似，并通过蒙特卡洛积分获得后验分布的经验估计和感兴趣的汇总统计量。其采样结果在计算机术语中也称为 trace。</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><strong>调试样本（Tune）</strong></p></td>
<td class="text-center"><p>调试样本是指在MCMC抽样过程中用于达到平稳分布的初始阶段，此阶段的样本通常被丢弃，不在最终分析中使用。也叫做 Warmup 或 burn-in，燃烧样本。</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>链（Chains）</strong></p></td>
<td class="text-center"><p>链是指来自单个MCMC链的一系列样本（或抽样值）。链用于诊断收敛性以及程序在特定应用中的其他潜在问题。</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>使用后验分布进行统计推断</strong></p>
<p>下面是我们使用PyMC等概率编程语言来实现模型推断的大致步骤：</p>
<p><img alt="alt text" src="../_images/image-52.png" /></p>
<p>🤔思考：</p>
<p>如何判断我们得到的mcmc结果是可靠的？通过mcmc得到的后验能告诉我们什么？</p>
</section>
<section id="markov-chain-diagnostics">
<h2>对马尔科夫链的诊断 (Markov chain diagnostics)<a class="headerlink" href="#markov-chain-diagnostics" title="Permalink to this heading">#</a></h2>
<p>🤔在使用我们的采样结果前需要思考一个问题，这些采样结果是否可信呢？</p>
<ul class="simple">
<li><p>由于这些采样来自于 MCMC 算法，即马尔科夫链蒙特卡洛算法，而模拟采样方法的关键是 “近似 “和 “收敛” 。也就是说，最终是否达到一个稳定的状态，表现为高概率的地方有更多的样本，低概率的地方对应更少的样本，并且是一种稳定的收敛的状态。</p></li>
</ul>
<p>我们需要考虑下面3个问题：</p>
<ul class="simple">
<li><p>好的马尔可夫链是什么样的？</p></li>
<li><p>如何判断马尔可夫链样本是否产生了合理的后验近似值？</p></li>
<li><p>马尔可夫链样本量应该多大？</p></li>
</ul>
<p><strong>诊断的意义</strong>：确定好的trace的标准，为我们不知道真实的后验时作为标准进行推断。</p>
<p>在本节中，我们将重点介绍几种诊断方法（这些诊断方法需要综合考虑！）：</p>
<ul class="simple">
<li><p>可视化诊断：轨迹图 (trace plot)和平行链 (parallel chains)。</p></li>
<li><p>量化诊断指标：有效样本大小 ESS(effective sample size)、自相关性 (autocorrelation)和<span class="math notranslate nohighlight">\(\widehat{R}\)</span>(R-hat)。</p></li>
</ul>
<section id="trace-plots">
<h3>轨迹图（trace plots）<a class="headerlink" href="#trace-plots" title="Permalink to this heading">#</a></h3>
<p>轨迹图 (trace plot) 是一种可视化方法，用于显示马尔可夫链的轨迹，即马尔可夫链在参数空间中的位置。</p>
<ul class="simple">
<li><p>图中的横坐标是时间步长（即每一次的采样），纵坐标是参数的值，Chain就是不同的链（绝大部分的PPL里默认的都是4条链）。</p></li>
<li><p>轨迹图往往结合参数的后验分布一起展示，以便于观察采样样本对于参数分布的代表性。</p></li>
</ul>
<p>我们先来看一下正常的轨迹图（之前 Beta-Binomial model 的示例）是什么样的：</p>
<p><img alt="alt text" src="../_images/image-63.png" /></p>
<ul class="simple">
<li><p>上图：轨迹图看起来像一堆白噪声，没有明显的趋势或现象（例如出现明显的跳跃或者隔断），这意味着<strong>链是稳定的</strong>。</p></li>
<li><p>下右：展示了正常的参数后验分布的情况。</p></li>
</ul>
<p>相比之下，我们再来看一个糟糕的轨迹图：</p>
<p><img alt="alt text" src="../_images/image-72.png" /></p>
<p><em><strong>图中上部分链 A：</strong></em></p>
<ul class="simple">
<li><p>链 A 中的轨迹图显示，它在 5000 次迭代后还没有稳定下来，并且它只探索了 0.6 到 0.9 之间的参数值。</p></li>
<li><p>下降趋势也暗示着链值之间存在着很强的相关性–它们看起来并不像独立的噪音。</p></li>
<li><p>虽然马尔可夫链本身具有依赖性，但它们越像噪声(独立样本的表现)，得出的后验近似值误差就越小（粗略地说）。</p></li>
<li><p>结合后验分布图：<strong>它的后验近似值高估了真实分布中央部分，而完全低估了这个范围之外值的可信度。</strong></p></li>
</ul>
<p><em><strong>图中下部分链 B：</strong></em></p>
<ul class="simple">
<li><p>链 B 表现出不同的问题，迹线图中的部分区域存在<strong>两条完全平直的线</strong>所显示的那样，</p></li>
<li><p>这意味着，当它采样到较小的参数值时，往往会<strong>陷入这个值的附近</strong>， 这也表明了一种局部的高相关性。</p></li>
<li><p>链条 B 在陷入困境时，会对后验参数左侧尾部的值进行<strong>过度采样</strong>。</p></li>
<li><p>结合密度图：
虽然链 B 后验分布和真实分布的重合性更好，但它存在多峰分布，原因在于过度采样。</p></li>
</ul>
<p>📍如果我们得到一个糟糕的轨迹图，如何进行补救：</p>
<ul class="simple">
<li><p>检查模型。确定假定的先验模型和数据模型是否合适。</p></li>
<li><p>对数据链进行更多迭代。一些不理想的短期连锁趋势可能会在长期内得到改善。</p></li>
</ul>
</section>
<section id="ess-effective-sample-size-autocorrelation">
<h3>有效样本大小 ESS(effective sample size)和自相关性 (autocorrelation)<a class="headerlink" href="#ess-effective-sample-size-autocorrelation" title="Permalink to this heading">#</a></h3>
<p>🤔在进行mcmc采样过程中，我们希望样本之间呈现什么样的特点？</p>
<ul class="simple">
<li><p>最理想的状态是每次采样都是相互独立的，这样的样本实际上能够更好地代表总体。例如，我们想要了解中国人的某个特点，如果我们全部从南师大进行抽样，可能样本之间在某些维度上存在高度相关，最后也就很难代表总体的特征。</p></li>
</ul>
<p>MCMC 链的样本存在相关性，如本身的特性一样，后一个参数的采样过程与前一个参数有关，这导致间隔一个位置的参数样本间的相关性很高。</p>
<p>然而，MCMC 链又要求“无记忆性”，即样本之间最好是独立的。</p>
<p>🤔那么如何理解 MCMC链中独立与相关的矛盾？</p>
<ul class="simple">
<li><p>需要清楚的是，相关性仅限于间隔一个位置的参数样本。</p></li>
<li><p>而独立性的要求是针对于参数样本间大于一个间隔的情况的。</p></li>
</ul>
<p><strong>自相关就可以很好的描述样本之间的相关性；而有效样本量针对于描述样本的独立性。</strong></p>
<p><strong>自相关性 (autocorrelation)</strong></p>
<ul class="simple">
<li><p>可用于评估马尔科夫链的样本之间的相关性。</p></li>
<li><p>强烈的自相关性或依赖性是一件坏事–<strong>它与较小的有效样本比相伴而生</strong>，说明我们得出的后验近似值可能不可靠。</p></li>
</ul>
<p>根据马尔可夫链的简单构造，链值之间必然存在一些自相关性：一个链值取决于前一个链值，而前一个链值又取决于更前一个链值…，以此类推。这种依赖性意味着，每个链值都在一定程度上依赖于之前的所有链值。</p>
<ul class="simple">
<li><p>然而，这种依赖性或自相关性会逐渐消失，这种相关会随着步长逐渐降低。</p></li>
</ul>
<blockquote>
<div><p>这就像托布勒的地理学第一定律：万事万物都与其他事物相关，<strong>但近处的事物比远处的事物更相关。</strong></p>
</div></blockquote>
<p>一个自相关良好的例子：</p>
<p><img alt="alt text" src="../_images/image-83.png" /></p>
<ul class="simple">
<li><p>图左是之前提到的轨迹图(trace plot)。</p></li>
<li><p>右图是自相关图，x轴是样本之间的间隔距离，y 轴为自相关性。</p></li>
<li><p>例如 0 代表所有参数采样和自己的相关性 (这显然为1)，1代表所有参数和间隔一个距离参数的样本的相关性，该相关性依然很高，大约为 0.5，表明相差仅 1 步的链值之间存在中等程度的相关性。随后自相关性迅速下降，<strong>到间隔 5 个样本时相关性下降为 0</strong>。</p></li>
<li><p>也就是说，相差间隔越大，马尔科夫链值之间的相关性越小，这是一个好消息，<strong>表明马尔科夫链正在快速混合</strong>，即在后验可信<span class="math notranslate nohighlight">\(π\)</span>值范围内快速移动，从而至少模拟了一个独立样本。</p></li>
<li><p>对于间隔解释的例子：如果有100个参数形成的样本，编号为 1-100。 那么间隔10，就代表，10-100组成的样本。为了让 10-100 和原始的样本长度相同，我们可以使用 1-90和 10-100 的样本进行相关分析。需要注意的是，存在更好的数学方法可以避免间隔越大，样本量越小的问题。扩展阅读: <a class="reference external" href="https://zhuanlan.zhihu.com/p/77072803">https://zhuanlan.zhihu.com/p/77072803</a></p></li>
</ul>
<p>一个自相关比较糟糕的例子：</p>
<p><img alt="alt text" src="../_images/image-93.png" /></p>
<ul class="simple">
<li><p>自相关曲线的缓慢下降表明，链值之间的依赖关系不会很快消失。</p></li>
<li><p>相差整整 20 步的马尔可夫链值之间存在大约 0.9 的相关性！</p></li>
<li><p>由于其链值与前值的联系非常紧密，因此该链的混合速度很慢，需要很长时间才能充分探索后验的全部范围。</p></li>
<li><p>因此，我们应该谨慎使用该链来近似后验。</p></li>
</ul>
</section>
<section id="fast-vs-slow-mixing-markov-chains">
<h3>重要概念：Fast vs slow mixing Markov chains<a class="headerlink" href="#fast-vs-slow-mixing-markov-chains" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>快速混合：指链表现出与独立样本的行为，即混合链值之间的自相关性迅速下降，有效样本大小比相当大。</p></li>
<li><p>慢速混合：指链不具有独立样本的特征，即混合链值之间的自相关性下降非常缓慢，有效样本大小比很小。换句话说，链在后验可信值范围内 “缓慢 “移动。</p></li>
</ul>
<p>📍<strong>如果我们的链混合速度太慢该怎么办呢？一些推荐的策略</strong>：</p>
<p>第一， “运行更长的链”。如果迭代次数足够多，即使是缓慢的混合链最终也能产生良好的后验近似值。</p>
<p>第二，使得马尔科夫链更薄。</p>
<ul class="simple">
<li><p>例如，在包含 5000 个样本的链中，我们每间隔一个参数来保留样本，并丢弃其余的样本，例如<span class="math notranslate nohighlight">\(\pi_2,\pi_4,...\pi_{5000}\)</span></p></li>
<li><p>我们还可以每间隔10个样本来保留：<span class="math notranslate nohighlight">\(\pi_{10},\pi_{20},...\pi_{5000}\)</span></p></li>
<li><p>通过舍弃中间的抽样，我们可以消除低间隔带来的强相关性。（<span class="math notranslate nohighlight">\(\pi_{20}\)</span>与稀疏链中的前一个值<span class="math notranslate nohighlight">\(\pi_{10}\)</span>的相关性小于与原始链中的前1个值<span class="math notranslate nohighlight">\(\pi_{19}的相关性\)</span>）</p></li>
</ul>
<p><strong>一个使 MCMC 链变得更薄(thin)来减少自相关的示例：</strong></p>
<p><img alt="alt text" src="../_images/image-103.png" /></p>
<ul class="simple">
<li><p>可以观察到，我们使用thin=10来将链变薄10倍，也就是从5000个样本中，每间隔10个样本保留1个，总共保留500个样本。</p></li>
<li><p>虽然自相关性快速下降，但这可能仍然无法完全消除自相关性。</p></li>
</ul>
<p>🤔需要思考一个问题：</p>
<ul class="simple">
<li><p>使用thin=10的时候，我们通过损失90%的原始样本值来降低自相关性，这样做是否值得？是不是一旦自相关性过高，我们就采用这种粗暴的方法来降低它？</p></li>
</ul>
<p>实际上，我们可以通过增加步长或迭代的次数来尝试解决这一问题，而不是一味地将链变薄。</p>
</section>
<section id="effective-sample-size-ratio">
<h3>有效样本量比率 Effective sample size ratio<a class="headerlink" href="#effective-sample-size-ratio" title="Permalink to this heading">#</a></h3>
<p>为了避免链中相关性的影响，有效样本量(effective sample size)被提出，以用于描述链中独立样本的数量。因为根据 MCMC 的特性，我们需要链存在<strong>无记忆性</strong>，也就是大于两个间隔的参数样本间不存在或者存在很小的相关性。</p>
<ul class="simple">
<li><p>有效样本量计算了这些不存在或者存在很小的相关性样本的数量。</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{N_{eff}}{N}
\]</div>
<ul class="simple">
<li><p>N 表示依赖马尔可夫链的实际样本量或长度。</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{eff}\)</span>为马尔可夫链的有效样本大小，量化了产生等效精确后验近似所需的独立样本数量。</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{eff}\)</span>越大越好，但马尔可夫链近似的准确性通常只能达到较小独立样本的准确性。也就是说，通常情况下，<span class="math notranslate nohighlight">\(N_{eff}&lt;N\)</span>,因此有效样本大小比＜1。我们通常会对有效样本量比＜0.1的马尔可夫链产生怀疑，即<span class="math notranslate nohighlight">\(N_{eff}/N&lt;0.1\)</span></p></li>
</ul>
<p>假设马尔科夫链的长度为 20,000，并且包括 6800 个独立样本，即有效样本量 ESS = 6800。</p>
<ul class="simple">
<li><p>因此，有效样本量比率 EFF=34%，代表相当于我们只使用了 34% 的独立样本。</p></li>
<li><p>由于这个比例高于 0.1，这是是可以接受的。并且 ESS &gt; 400(ESS&gt;400是近些年研究者推荐的一个大致标准)。</p></li>
</ul>
<blockquote>
<div><p>更详细的定义请参见 Vehtari 等人（2021）。如需了解有效样本量, R-hat等指标的的联系，请参阅 Vats 和 Knudson (2018)。</p>
</div></blockquote>
</section>
<section id="r-hat-and-comparing-parallel-chains">
<h3>R-hat and comparing parallel chains<a class="headerlink" href="#r-hat-and-comparing-parallel-chains" title="Permalink to this heading">#</a></h3>
<p>自相关和有效样本量仅针对于描述一条MCMC链的有效性。假如我们获得了四条并行的马尔可夫链。我们不仅希望看到每条链的稳定性，还希望看到<strong>四条链的一致性</strong>。</p>
<p>可以想象，如果四条链的结果并不一致，那么推断结果的可靠性就很难保证。</p>
<p>同样，我们首先展示一个良好的并行链的结果：</p>
<p><img alt="alt text" src="../_images/image-112.png" /></p>
<ul class="simple">
<li><p>四条链产生的后验近似值几乎没有差别，这证明我们的模拟是稳定的。（也就是说，这4条链虽然起点不一样，但是最后的形状几乎混合在一起，说明mcmc的指标比较良好）</p></li>
</ul>
<p>为了便于比较和展示一个糟糕的并行链的结果，我们用更短的马尔可夫链模拟来演示：</p>
<p><img alt="alt text" src="../_images/image-122.png" /></p>
<ul class="simple">
<li><p>运行四条并行链仅进行 100 次迭代，其中 50 次为warmup (burn-in)</p></li>
<li><p>虽然这些链的轨迹图表现出相似的随机行为，但它们对应的密度图却不尽相同，因此产生的后验近似值也不尽相同。</p></li>
</ul>
<p>这表明，如果我们只进行了 100 次迭代就停止模拟，那结果是不稳的。</p>
<p><strong>不同链样本之间的变异性的一致性也很重要：</strong></p>
<p><img alt="alt text" src="../_images/image-112.png" /></p>
<ul class="simple">
<li><p>中图显示，4条链的可变性几乎是相同的。右图中，所有链的变异与综合为单个链的变异性相似。</p></li>
</ul>
<p>我们再来看一个糟糕的例子：</p>
<p><img alt="alt text" src="../_images/image-132.png" /></p>
<ul class="simple">
<li><p>观察轨迹图（左图）可以发现，4条链的范围均不一致，并且涉及的变异性也存在较明显的差异。</p></li>
<li><p>四条平行链产生了相互冲突的后验近似值（中图）</p></li>
<li><p>当我们将这些链组合在一起时，后验近似值不稳定且较差（右图）</p></li>
<li><p>因此，所有链中参数值的范围和变异性远大于任何单个链中参数值的范围和变异性</p></li>
</ul>
<p>我们可以直观地看出，所有链条上的数值变化与单个并行链条内部的数值变化之间的关系非常重要。具体来说</p>
<ul class="simple">
<li><p>在 <strong>“好的 “马尔可夫链模拟</strong>中，所有<strong>并行链的变异性总和</strong>将与任何<strong>单个链内的变异性</strong>大致相当；</p></li>
<li><p>在 <strong>“坏的 “马尔可夫链模拟</strong>中，所有并行链的变异性总和可能<strong>超过每个链内的典型变异性</strong></p></li>
</ul>
<p>我们可以使用<span class="math notranslate nohighlight">\(R̂\)</span>量化<strong>综合链变异性和链内变异性</strong>之间的关系。</p>
<p><span class="math notranslate nohighlight">\(R̂\)</span>指标（R-hat）为并行链的可视化比较提供了具体的数学指标：</p>
<div class="math notranslate nohighlight">
\[
R̂ = \frac{V_{combined}}{V_{within}}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R̂\)</span>通过比较<strong>所有链</strong>上采样值的变异性和每个<strong>单独链</strong>上的变异性来解决一致性问题。</p></li>
<li><p>combined代表所有链的样本值的总变异，within代表每个链内部样本值的变异。</p></li>
</ul>
<p>理想情况下，<span class="math notranslate nohighlight">\(R̂\)</span>≈1，反映了平行链的稳定性。一般来说，我们认为越接近1越好</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R̂&gt;1\)</span>表示链之间的变异不稳定，即组合链内的变异性大于单个链内的变异性。</p></li>
<li><p>虽然不存在黄金法则，但大于1.05会给模拟的稳定性带来危险信号，此时需要谨慎考虑。如果更大，例如大于1.1或1.2，需要重新进行考虑，此时审稿人也可能会表示质疑。</p></li>
</ul>
<p>对于示例1：</p>
<p><img alt="alt text" src="../_images/image-112.png" /></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R̂=1\)</span>，这反映了4个并行链之间和链内部的变异性存在很好的一致性。</p></li>
</ul>
<p>对于示例2：</p>
<p><img alt="alt text" src="../_images/image-132.png" /></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R̂=5.35\)</span>，这表明所有链值的方差总和是每个链内方差的5倍多。这远远超过了1.05的标准，充分证明平行链并未产生一致的后验近似值。</p></li>
</ul>
<p>因此，我们可以得出，mcmc的模拟近似是不稳定的，其后验分布的结果不可用于后续推断。</p>
</section>
</section>
<section id="id1">
<h2>代码演示–MCMC诊断<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>使用 PyMC 建模的模型默认使用 MCMC 方法进行参数估计。</p>
<p>并且我们可以很方便的使用 arviz 对MCMC进行可视化和基于数学化标准的诊断</p>
<p>首先，我们建立一个简单的 beta 模型，并对该模型使用MCMC采样方法 (以下术语可见课件上分术语表)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># 设置数据</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="n">Y</span> <span class="o">=</span> <span class="mi">9</span>
<span class="c1"># 创建PyMC模型</span>
<span class="n">bb_model</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>

<span class="k">with</span> <span class="n">bb_model</span><span class="p">:</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;pi&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pi</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

<span class="c1">#采样过程仍在该容器中进行</span>
<span class="k">with</span> <span class="n">bb_model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>                   <span class="c1"># 设置MCMC采样样本的数量</span>
                      <span class="n">tune</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>                       <span class="c1"># 设置调试样本的数量</span>
                      <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>                     <span class="c1"># 设置4条MCMC链</span>
                      <span class="n">random_seed</span><span class="o">=</span><span class="mi">84735</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>设置采样样本数 (draws) 为 5000</p></li>
<li><p>设置 MCMC 链 (chains) 的数量为 4，和上面的示例一样。</p></li>
<li><p>设置 MCMC 调试样本的数量 (tune) 为0，而默认 tune 为 1000。</p></li>
<li><p>这里暂时理解 tune 为提前进行 MCMC 采样 1000次，然后丢弃掉这些采样。</p></li>
<li><p>根据 MCMC 的性质，<strong>链中越早的采样越不稳定</strong>。因此，丢掉这些采样可以保证后续的采样更加稳定。
后续课程会详细介绍该参数的意义。</p></li>
</ul>
<p><strong>绘制 trace 图</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="alt text" src="../_images/image-142.png" /></p>
<ul class="simple">
<li><p>可以看到，每条链的后验分布都重合在一起。</p></li>
<li><p>MCMC链的行为表现得类似噪音一样，这是一个好的表现。</p></li>
</ul>
<p><strong>绘制自相关图</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_autocorr</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">max_lag</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="alt text" src="../_images/image-151.png" /></p>
<ul class="simple">
<li><p>可以看到，随着间隔的下降，自相关也快速下降</p></li>
</ul>
<p><strong>我们使用 arviz 的 summary来获得对于 MCMC 链的 R-hat和 有效样本量 ESS 指标</strong></p>
<ul class="simple">
<li><p>可以看到 r-hat 接近1，表明链间和各链内的变异的一致性。</p></li>
<li><p>有效样本量 ESS (ess_bulk) 为 4744，有效样本量比率 4744/5000 = 0.95，远大于 0.1。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;diagnostics&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="alt text" src="../_images/image-161.png" /></p>
<ul class="simple">
<li><p>根据前面的内容，我们通过mcmc找到了后验，首先需要诊断这个后验是否靠谱，接下来则是用这个后验分布帮助我们解决实际感兴趣的问题，也就是需要进行一些统计推断。</p></li>
</ul>
</section>
<section id="posterior-estimation">
<h2>Posterior estimation<a class="headerlink" href="#posterior-estimation" title="Permalink to this heading">#</a></h2>
<p>当我们确定 MCMC 采样的结果的可信后，我们就可以将其当作后验分布进行统计推断。</p>
<p>在此之前，我们需要回顾后验分布的含义和意义。</p>
<ul class="simple">
<li><p>在贝叶斯的框架下，我们可以把后验分布看作是对成功次数<span class="math notranslate nohighlight">\(\pi\)</span>的一种估计，即当前数据更可能在哪一种<span class="math notranslate nohighlight">\(\pi\)</span>下出现。</p></li>
</ul>
<p>🤔当你看到下面这个后验分布时，你觉得它描述了关于<span class="math notranslate nohighlight">\(\pi\)</span>怎样的一种信息呢？</p>
<p>a. 参与者正确判断的概率大概在80%</p>
<p>b. 参与者正确判断的概率比例最有可能是80%，但这个比例也可能是60%到99%中的某一个。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 导入数字和向量处理包：numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 导入基本绘图工具：matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># 导入高级绘图工具 seaborn 为 sns</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="c1"># 导入概率分布计算和可视化包：preliz</span>
<span class="kn">import</span> <span class="nn">preliz</span> <span class="k">as</span> <span class="nn">pz</span>

<span class="k">def</span> <span class="nf">bayesian_analysis_plot</span><span class="p">(</span>
    <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> 
    <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">plot_prior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">plot_likelihood</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">plot_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\pi$&quot;</span><span class="p">,</span> 
    <span class="n">show_legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">legend_loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    该函数绘制先验分布、似然分布和后验分布的 PDF 图示在指定的子图上。</span>
<span class="sd">    </span>
<span class="sd">    参数:</span>
<span class="sd">    - alpha: Beta 分布的 alpha 参数（先验）</span>
<span class="sd">    - beta: Beta 分布的 beta 参数（先验）</span>
<span class="sd">    - y: 观测数据中的支持次数</span>
<span class="sd">    - n: 总样本数</span>
<span class="sd">    - ax: 子图对象，在指定子图上绘制图形</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">plot_prior</span><span class="p">:</span>
        <span class="c1"># 先验分布</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="n">pz</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">prior</span><span class="o">.</span><span class="n">plot_pdf</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>
        <span class="n">x_prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">prior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span> <span class="n">prior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.9999</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_prior</span><span class="p">,</span> <span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_prior</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#f0e442&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_likelihood</span><span class="p">:</span>
        <span class="c1"># 似然分布 (两种写法等价)</span>
        <span class="c1"># likelihood = pz.Beta(y,n-y)</span>
        <span class="c1"># likelihood.plot_pdf(color=&quot;black&quot;, ax=ax, legend=&quot;None&quot;)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pz</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">y</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">n</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\mathbf</span><span class="si">{Binomial}</span><span class="s2">$&quot;</span><span class="o">+</span><span class="sa">rf</span><span class="s2">&quot;(n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">,p=</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">y</span><span class="o">/</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#0071b2&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;likelihood&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_posterior</span><span class="p">:</span>
        <span class="c1"># 后验分布</span>
        <span class="n">posterior</span> <span class="o">=</span> <span class="n">pz</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">posterior</span><span class="o">.</span><span class="n">plot_pdf</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>
        <span class="n">x_posterior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span> <span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.9999</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_posterior</span><span class="p">,</span> <span class="n">posterior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_posterior</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#009e74&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">show_legend</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">legend_loc</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># 设置图形</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 创建一个单独的图和轴</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># 先验参数 alpha=2, beta=2, 观测数据 y=9, n=10</span>
<span class="n">bayesian_analysis_plot</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span> 

<span class="c1"># 显示图像</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="alt text" src="../_images/image-171.png" /></p>
<p>选项b是更符合贝叶斯取向的回答。 为什么？ ➡</p>
<ul class="simple">
<li><p>我们来考虑另一种调查情况下得出的后验分布：</p></li>
<li><p>我们对<span class="math notranslate nohighlight">\(\pi\)</span>的先验仍然为<span class="math notranslate nohighlight">\(\pi \sim Beta(2,2)\)</span>，但现在我们的试验总次数为118次，成功次数为99次。</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\pi \sim Beta(2,2)
\]</div>
<div class="math notranslate nohighlight">
\[
Y|\pi \sim Bin(118,\pi)
\]</div>
<div class="math notranslate nohighlight">
\[
\pi|(y=99) \sim Beta(\alpha+y,\beta +n-y)
\]</div>
<p>这种情况下，<span class="math notranslate nohighlight">\(\pi\)</span>的后验分布满足<span class="math notranslate nohighlight">\(\pi|(Y=99) \sim Beta(101,21)\)</span></p>
<p>下图展示了这两种情况下的后验分布</p>
<blockquote>
<div><p>黑线表示众数，即在后验分布中最可能出现的值</p>
</div></blockquote>
<p><img alt="alt text" src="../_images/image-181.png" /></p>
<p>根据上面两种分布，<span class="math notranslate nohighlight">\(\pi\)</span>最可能的取值都在83%-84%左右</p>
<ul class="simple">
<li><p>如果只关注众数，两种截然不同的情况却会导致相似的结论。但很明显，我们可以看到两个图中<span class="math notranslate nohighlight">\(\pi\)</span>的分布范围是不同的。</p></li>
<li><p>但是，随着样本量的增加，后验分布会越来越窄，也会朝着这个集中量数越来越集中，这<strong>意味着你的不确定性在慢慢被消除</strong>。</p></li>
</ul>
<p>因此，在贝叶斯统计中，<strong>汇报不确定性</strong>是一个非常重要的工作。不确定性反映了你的数据量到底有多大，你的信心到底有多大。</p>
<p><strong>为了同时了解<span class="math notranslate nohighlight">\(\pi\)</span>的集中趋势和离散趋势，我们可以使用95%的可信区间(credible intervals)来描述<span class="math notranslate nohighlight">\(\pi\)</span></strong></p>
<section id="credible-intervals">
<h3>可信区间(credible intervals)<a class="headerlink" href="#credible-intervals" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>后验分布的2.5%百分位数和97.5%百分位数包含的区域共同组成了95%的后验可信区间，可信区间表示了参数出现在这个区域的概率。</p></li>
<li><p>比如，Beta(101, 21) 95%的可信区间为(0.756, 0.889)。这意味着我们可以说，在随机点运动任务中参与者成功的真实概率出现在(0.756, 0.889)的可能性是95%。</p></li>
</ul>
<p>一种理解是，有95%的人在随机点运动任务中参与者成功概率很高，大概为 0.756 到 0.889。</p>
<p>下面是可视化的例子：</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c1"># 定义 Beta 分布的参数 Beta(101, 21)</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">101</span><span class="p">,</span> <span class="mi">21</span><span class="p">)]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;Beta(11, 3)&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;Beta(101, 21)&quot;</span><span class="p">]</span>
<span class="n">ci_level</span> <span class="o">=</span> <span class="mf">0.95</span>

<span class="c1"># 创建图形和子图</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 绘制 Beta 分布的 PDF 和 95% CI 区间</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">([(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ci_level</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">ci_level</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">],</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># 绘图</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">ci_level</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s1">% CI&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#a6bddb&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\pi$&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;density&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>  
    
    <span class="c1"># 隐藏上部和右部的边框</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="c1"># 添加图例</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># 调整子图布局</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="alt text" src="../_images/image-191.png" /></p>
<p>注意：贝叶斯的可信区间和频率主义的置信区间解读不同，不要混淆这两个概念。</p>
<p><strong>在报告可信区间时，95%是最常见的选项，但并不是唯一的选项。</strong></p>
<p>我们也可以创建其他可信区间，比如Beta(101, 21)的分布之下：</p>
<ul class="simple">
<li><p>50% 可信区间：由25%百分位数和75%百分位数包含的区域</p></li>
<li><p>99%可信区间：由0.5%百分位数和99.5%百分位数包含的区域</p></li>
</ul>
<blockquote>
<div><p>在95%CI的图中，由于后验分布是一个偏态分布，可以看到相较于前半部分，后半部分包含的值对应的概率更低。
当后验分布是偏态分布时，有时我们不以中位数为中心构建95%的可信区间，而是基于<strong>最高后验概率密度(众数)</strong>。</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c1"># 定义 Beta 分布的参数 Beta(101, 21)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">21</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># 设置三个不同的可信区间</span>
<span class="n">ci_levels</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;50% CI&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">),</span>
    <span class="s2">&quot;95% CI&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">),</span>
    <span class="s2">&quot;99% CI&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">ci_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#bdd7e7&#39;</span><span class="p">,</span> <span class="s1">&#39;#6baed6&#39;</span><span class="p">,</span> <span class="s1">&#39;#3182bd&#39;</span><span class="p">]</span>  <span class="c1"># Colors for 50%, 95%, and 99% CI</span>

<span class="c1"># 为每个可信区间创建子图</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 绘制每个可信区间</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ci_name</span><span class="p">,</span> <span class="p">(</span><span class="n">lower_percentile</span><span class="p">,</span> <span class="n">upper_percentile</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ci_levels</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="c1"># 计算可信区间的上下界</span>
    <span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="n">lower_percentile</span><span class="p">,</span> <span class="n">upper_percentile</span><span class="p">],</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># 绘制分布曲线并填充可信区间</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">ci_lower</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">ci_upper</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">ci_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ci_name</span><span class="si">}</span><span class="s2"> for Beta(101, 21)&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\pi$&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
    
    <span class="c1"># 绘制可信区间的垂直线</span>
    <span class="n">y_ci_lower</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">y_ci_upper</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">ci_upper</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">y_ci_lower</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">ci_upper</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">y_ci_upper</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    
    <span class="c1"># 移除顶部和右侧的边框</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># 设置 y 轴标签</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;density&quot;</span><span class="p">)</span>

<span class="c1"># 调整布局</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="alt text" src="../_images/image-20.png" /></p>
</section>
<section id="the-highest-density-interval-hdi">
<h3>最大后验概率密度区间(the highest density interval, HDI)<a class="headerlink" href="#the-highest-density-interval-hdi" title="Permalink to this heading">#</a></h3>
<p>最大概率密度区间HDI与中心区间是非常接近的。但是<strong>当后验分布为非正态分布时，两者存在区别</strong>，比如：</p>
<p><img alt="alt text" src="../_images/image-211.png" /></p>
<ul class="simple">
<li><p>左图是中央后验区间，从中位数的两侧开始展开</p></li>
<li><p>右图是HDI，从y轴最高的两处（众数）开始展开</p></li>
</ul>
<p>这个HDI是现在大家常见的并且比较推荐的一种报告方法。</p>
<p><em><strong>直觉理解：想象有一根水平的线从后验概率密度最高点往下滑，如果是95%HDI，则意味着黄色部分的面积占曲线下面积的95%</strong></em></p>
<p><img alt="alt text" src="../_images/image-22.png" /></p>
<blockquote>
<div><p>source: <a class="reference external" href="https://mathematica.stackexchange.com/questions/173282/computing-credible-region-highest-posterior-density-from-empirical-distributio">https://mathematica.stackexchange.com/questions/173282/computing-credible-region-highest-posterior-density-from-empirical-distributio</a></p>
</div></blockquote>
<p>🤔思考：为什么要使用HDI，而不是中央后验区间呢？</p>
<p>大家可以看到，如果我们要寻找一个范围，这个范围最能够代表参数取值的话，那么应该从密度高的地方开始取值，也就是可能性最大的地方开始取值。当后验分布是非正态时，中位数并不是密度最高的地方，因此我们选择HDI进行取值，这更符合我们想要的一个结果。</p>
<ul class="simple">
<li><p>从下面的两幅图可以看出，95% CI 和 95%HDI实际上存在一些细微的差别。</p></li>
</ul>
<p><img alt="alt text" src="../_images/image-23.png" /></p>
<p>此外，非正态分布后验的平均值，中位数和众数并不相等。</p>
<ul class="simple">
<li><p>其中众数为分布最高点对应的参数值</p></li>
<li><p>中位数左右两侧分布的面积各占50%</p></li>
<li><p>平均值的位置接近中位数，并且它容易分布形态的影响</p></li>
</ul>
<p><img alt="alt text" src="../_images/image-24.png" /></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter6"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter6_part1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture7 : Posterior Inference &amp; Estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter7/chapter7_part1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><center> Lecture8 : A Simple Linear Regression Model with PyMC </center></p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ppl-probabilistic-programming-language">什么是概率编程语言（PPL，probabilistic programming language）</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-diagnostics">对马尔科夫链的诊断 (Markov chain diagnostics)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trace-plots">轨迹图（trace plots）</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ess-effective-sample-size-autocorrelation">有效样本大小 ESS(effective sample size)和自相关性 (autocorrelation)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fast-vs-slow-mixing-markov-chains">重要概念：Fast vs slow mixing Markov chains</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effective-sample-size-ratio">有效样本量比率 Effective sample size ratio</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-hat-and-comparing-parallel-chains">R-hat and comparing parallel chains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">代码演示–MCMC诊断</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-estimation">Posterior estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#credible-intervals">可信区间(credible intervals)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-highest-density-interval-hdi">最大后验概率密度区间(the highest density interval, HDI)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr. Hu Chuan-Peng
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>